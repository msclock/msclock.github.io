<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.7.2/css/all.min.css" integrity="sha256-dABdfBfUoC8vJUBOwGVdm8L9qlMWaHTIfXt+7GnZCIo=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/themes/blue/pace-theme-center-circle.css">
  <script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous" defer></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"msclock.github.io","root":"/","images":"/images","scheme":"Pisces","darkmode":true,"version":"8.23.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":true,"style":"github-dark"},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":true,"lazyload":true,"pangu":true,"comments":{"style":"tabs","active":"livere","storage":true,"lazyload":false,"nav":null,"activeClass":"livere"},"stickytabs":true,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false,"trigger":"auto"}}</script><script src="https://cdn.jsdelivr.net/npm/hexo-theme-next@8.23.0/source/js/config.min.js" defer></script>

    <meta property="og:type" content="article">
<meta property="og:title" content="AI 实践">
<meta property="og:url" content="https://msclock.github.io/posts/c21efe28/index.html">
<meta property="og:site_name" content="Live in the present">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2022-08-28T20:30:45.000Z">
<meta property="article:modified_time" content="2025-04-16T17:13:34.592Z">
<meta property="article:author" content="Msclock">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="Triton Serving">
<meta property="article:tag" content="AI Serving">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://msclock.github.io/posts/c21efe28/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://msclock.github.io/posts/c21efe28/","path":"posts/c21efe28/","title":"AI 实践"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>AI 实践 | Live in the present</title>
  








  
  <script size="300" alpha="0.6" zIndex="-1" src="https://cdn.jsdelivr.net/npm/ribbon.js@1.0.2/dist/ribbon.min.js" defer></script>
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous" defer></script>
  <script src="https://cdn.jsdelivr.net/npm/@next-theme/pjax@0.6.0/pjax.min.js" integrity="sha256-vxLn1tSKWD4dqbMRyv940UYw4sXgMtYcK6reefzZrao=" crossorigin="anonymous" defer></script>
  <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous" defer></script>
  <script src="https://cdn.jsdelivr.net/npm/lozad@1.16.0/dist/lozad.min.js" integrity="sha256-mOFREFhqmHeQbXpK2lp4nA3qooVgACfh88fpJftLBbc=" crossorigin="anonymous" defer></script>
  <script src="https://cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js" integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin="anonymous" defer></script>
<script src="https://cdn.jsdelivr.net/npm/hexo-theme-next@8.23.0/source/js/utils.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/hexo-theme-next@8.23.0/source/js/motion.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/hexo-theme-next@8.23.0/source/js/sidebar.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/hexo-theme-next@8.23.0/source/js/next-boot.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/hexo-theme-next@8.23.0/source/js/pjax.min.js" defer></script>

  <script src="https://cdn.jsdelivr.net/npm/hexo-generator-searchdb@1.4.1/dist/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous" defer></script>
<script src="https://cdn.jsdelivr.net/npm/hexo-theme-next@8.23.0/source/js/third-party/search/local-search.min.js" defer></script>

  <script class="next-config" data-name="pdf" type="application/json">{"object_url":{"url":"https://cdn.jsdelivr.net/npm/pdfobject@2.3.1/pdfobject.min.js","integrity":"sha256-jI72I8ZLVflVOisZIOaLvRew3tyvzeu6aZXFm7P7dEo="},"url":"/lib/pdf/web/viewer.html"}</script>
  <script src="https://cdn.jsdelivr.net/npm/hexo-theme-next@8.23.0/source/js/third-party/tags/pdf.min.js" defer></script>

  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"neutral","dark":"neutral"},"js":{"url":"https://cdn.jsdelivr.net/npm/mermaid@11.5.0/dist/mermaid.min.js","integrity":"sha256-2obLuIPcceEhkE3G09G33hBdmE55ivVcZUlcKcGNHjU="}}</script>
  <script src="https://cdn.jsdelivr.net/npm/hexo-theme-next@8.23.0/source/js/third-party/tags/mermaid.min.js" defer></script>



  <script src="https://cdn.jsdelivr.net/npm/hexo-theme-next@8.23.0/source/js/third-party/pace.min.js" defer></script>


  




  <script src="https://cdn.jsdelivr.net/npm/quicklink@2.3.0/dist/quicklink.umd.js" integrity="sha256-yvJQOINiH9fWemHn0vCA5lsHWJaHs6/ZmO+1Ft04SvM=" crossorigin="anonymous" defer></script>
  <script class="next-config" data-name="quicklink" type="application/json">{"enable":true,"home":true,"archive":true,"delay":true,"timeout":3000,"priority":true,"url":"https://msclock.github.io/posts/c21efe28/"}</script>
  <script src="https://cdn.jsdelivr.net/npm/hexo-theme-next@8.23.0/source/js/third-party/quicklink.min.js" defer></script>

  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<style>.darkmode--activated{--body-bg-color:#282828;--content-bg-color:#333;--card-bg-color:#555;--text-color:#ccc;--blockquote-color:#bbb;--link-color:#ccc;--link-hover-color:#eee;--brand-color:#ddd;--brand-hover-color:#ddd;--table-row-odd-bg-color:#282828;--table-row-hover-bg-color:#363636;--menu-item-bg-color:#555;--btn-default-bg:#222;--btn-default-color:#ccc;--btn-default-border-color:#555;--btn-default-hover-bg:#666;--btn-default-hover-color:#ccc;--btn-default-hover-border-color:#666;--highlight-background:#282b2e;--highlight-foreground:#a9b7c6;--highlight-gutter-background:#34393d;--highlight-gutter-foreground:#9ca9b6}.darkmode--activated img{opacity:.75}.darkmode--activated img:hover{opacity:.9}.darkmode--activated code{color:#69dbdc;background:0 0}button.darkmode-toggle{z-index:9999}.darkmode-ignore,img{display:flex!important}.beian img{display:inline-block!important}</style></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Live in the present</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Msclock's Notes</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">32</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">2</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">50</span></a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="搜索..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#AI-%E8%B5%84%E6%BA%90"><span class="nav-number">1.</span> <span class="nav-text">AI 资源</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Beginner"><span class="nav-number">2.</span> <span class="nav-text">Beginner</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9C%BA%E5%99%A8%E5%A6%82%E4%BD%95%E5%AD%A6%E4%B9%A0"><span class="nav-number">2.1.</span> <span class="nav-text">机器如何学习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Colab"><span class="nav-number">2.2.</span> <span class="nav-text">Colab</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ipynb-notebook"><span class="nav-number">2.3.</span> <span class="nav-text">ipynb notebook</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%BD%AC%E4%B8%BA%E6%96%87%E4%BB%B6"><span class="nav-number">2.3.1.</span> <span class="nav-text">转为文件</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Math-in-AI"><span class="nav-number">2.4.</span> <span class="nav-text">Math in AI</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="nav-number">2.5.</span> <span class="nav-text">激活函数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#sigmoid"><span class="nav-number">2.5.1.</span> <span class="nav-text">sigmoid</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Palm-Detection"><span class="nav-number">2.6.</span> <span class="nav-text">Palm Detection</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#NHWC-to-NCHW"><span class="nav-number">2.7.</span> <span class="nav-text">NHWC to NCHW</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ml-in-c"><span class="nav-number">3.</span> <span class="nav-text">ml in c</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#AI-copilot"><span class="nav-number">4.</span> <span class="nav-text">AI copilot</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#cursor"><span class="nav-number">4.1.</span> <span class="nav-text">cursor</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#connect-to-wsl"><span class="nav-number">4.2.</span> <span class="nav-text">connect to wsl</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#cocopilot"><span class="nav-number">4.3.</span> <span class="nav-text">cocopilot</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#open-interpreter"><span class="nav-number">4.4.</span> <span class="nav-text">open-interpreter</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#numpy"><span class="nav-number">5.</span> <span class="nav-text">numpy</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Tips"><span class="nav-number">5.1.</span> <span class="nav-text">Tips</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A2%9E%E5%88%A0%E7%BB%B4%E5%BA%A6"><span class="nav-number">5.1.1.</span> <span class="nav-text">增删维度</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#NHWC-to-NCHW-2"><span class="nav-number">5.2.</span> <span class="nav-text">NHWC to NCHW</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#pandas"><span class="nav-number">6.</span> <span class="nav-text">pandas</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#api"><span class="nav-number">6.1.</span> <span class="nav-text">api</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#quantile"><span class="nav-number">6.1.1.</span> <span class="nav-text">quantile</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ONNX"><span class="nav-number">7.</span> <span class="nav-text">ONNX</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#OpenCV"><span class="nav-number">8.</span> <span class="nav-text">OpenCV</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%89%E8%A3%85"><span class="nav-number">8.1.</span> <span class="nav-text">安装</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Tip"><span class="nav-number">8.2.</span> <span class="nav-text">Tip</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#jpg-to-numpy"><span class="nav-number">8.2.1.</span> <span class="nav-text">jpg to numpy</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#GPT"><span class="nav-number">9.</span> <span class="nav-text">GPT</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%B5%84%E6%BA%90"><span class="nav-number">9.1.</span> <span class="nav-text">资源</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9E%E7%94%A8%E5%B7%A5%E5%85%B7"><span class="nav-number">9.2.</span> <span class="nav-text">实用工具</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Stable-Diffusion"><span class="nav-number">9.3.</span> <span class="nav-text">Stable Diffusion</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#gptcommit"><span class="nav-number">9.4.</span> <span class="nav-text">gptcommit</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#prompt"><span class="nav-number">9.5.</span> <span class="nav-text">prompt</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#AI-Workflow"><span class="nav-number">10.</span> <span class="nav-text">AI Workflow</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#AI-Serving"><span class="nav-number">11.</span> <span class="nav-text">AI Serving</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#AI-Serving-Solution"><span class="nav-number">11.1.</span> <span class="nav-text">AI Serving Solution</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#API%E6%9C%8D%E5%8A%A1%E5%B7%A5%E5%85%B7"><span class="nav-number">11.1.1.</span> <span class="nav-text">API 服务工具</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#AI-Dev-%E8%BE%85%E5%8A%A9%E6%A1%86%E6%9E%B6"><span class="nav-number">11.1.2.</span> <span class="nav-text">AI Dev 辅助框架</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#TensorFlow-Serving"><span class="nav-number">11.2.</span> <span class="nav-text">TensorFlow Serving</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%83%BD%E5%90%A6%E7%BB%99%E4%B8%80%E4%B8%AA%E4%BD%BF%E7%94%A8-TensorFlow-Serving-%E5%B0%86%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E4%B8%BA%E6%9C%8D%E5%8A%A1%E7%9A%84%E7%A4%BA%E4%BE%8B"><span class="nav-number">11.2.1.</span> <span class="nav-text">能否给一个使用 TensorFlow Serving 将机器学习模型部署为服务的示例</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Torch-Serving"><span class="nav-number">11.3.</span> <span class="nav-text">Torch Serving</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%83%BD%E5%90%A6%E7%BB%99%E4%B8%80%E4%B8%AA%E4%BD%BF%E7%94%A8-Torch-Serving-%E5%B0%86%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E4%B8%BA%E6%9C%8D%E5%8A%A1%E7%9A%84%E7%A4%BA%E4%BE%8B"><span class="nav-number">11.3.1.</span> <span class="nav-text">能否给一个使用 Torch Serving 将机器学习模型部署为服务的示例</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Triton-Serving"><span class="nav-number">11.4.</span> <span class="nav-text">Triton Serving</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8Triton-Serving%E9%83%A8%E7%BD%B2AI%E6%9C%8D%E5%8A%A1"><span class="nav-number">11.4.1.</span> <span class="nav-text">如何使用 Triton Serving 部署 AI 服务</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Airflow"><span class="nav-number">11.5.</span> <span class="nav-text">Airflow</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A6%82%E4%BD%95%E7%94%A8Airflow%E6%A1%86%E6%9E%B6%E5%AE%9E%E7%8E%B0%E6%A8%A1%E5%9E%8B%E5%B8%B8%E9%A9%BB%E5%9C%A8%E5%86%85%E5%AD%98%E7%9A%84AI%E6%9C%8D%E5%8A%A1"><span class="nav-number">11.5.1.</span> <span class="nav-text">如何用 Airflow 框架实现模型常驻在内存的 AI 服务</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Airflow%E8%83%BD%E5%90%A6%E6%89%8B%E5%8A%A8%E9%85%8D%E7%BD%AE%E8%BF%90%E8%A1%8C%E8%BF%99%E4%B8%AAAI%E6%9C%8D%E5%8A%A1%E7%9A%84%E6%95%B0%E9%87%8F"><span class="nav-number">11.5.2.</span> <span class="nav-text">Airflow 能否手动配置运行这个 AI 服务的数量</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Dapr"><span class="nav-number">11.6.</span> <span class="nav-text">Dapr</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A6%82%E4%BD%95%E7%94%A8Dapr%E6%A1%86%E6%9E%B6%E5%AE%9E%E7%8E%B0%E6%A8%A1%E5%9E%8B%E5%B8%B8%E9%A9%BB%E5%9C%A8%E5%86%85%E5%AD%98%E7%9A%84AI%E6%9C%8D%E5%8A%A1"><span class="nav-number">11.6.1.</span> <span class="nav-text">如何用 Dapr 框架实现模型常驻在内存的 AI 服务</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Dapr%E8%83%BD%E5%90%A6%E6%89%8B%E5%8A%A8%E9%85%8D%E7%BD%AE%E8%BF%90%E8%A1%8C%E4%B8%8A%E9%9D%A2%E6%8F%90%E5%88%B0%E7%9A%84AI%E6%9C%8D%E5%8A%A1%E6%95%B0%E9%87%8F"><span class="nav-number">11.6.2.</span> <span class="nav-text">Dapr 能否手动配置运行上面提到的 AI 服务数量</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%83%BD%E5%90%A6%E5%B0%86tensorflow-serving-%E6%9B%BF%E6%8D%A2%E4%B8%BAtorch-serving"><span class="nav-number">11.6.3.</span> <span class="nav-text">能否将 tensorflow serving 替换为 torch serving</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#TensorFlow-Serving-%E5%92%8C-Torch-Serving-%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="nav-number">11.6.4.</span> <span class="nav-text">TensorFlow Serving 和 Torch Serving 的区别</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#AI-serving-app"><span class="nav-number">11.7.</span> <span class="nav-text">AI serving app</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#flask-app"><span class="nav-number">11.7.1.</span> <span class="nav-text">flask app</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Kubeflow"><span class="nav-number">12.</span> <span class="nav-text">Kubeflow</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#install"><span class="nav-number">12.1.</span> <span class="nav-text">install</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#KServe"><span class="nav-number">13.</span> <span class="nav-text">KServe</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Quickstart"><span class="nav-number">13.1.</span> <span class="nav-text">Quickstart</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#modelmesh-installing"><span class="nav-number">13.2.</span> <span class="nav-text">modelmesh installing</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Demo-Test"><span class="nav-number">13.3.</span> <span class="nav-text">Demo Test</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#TAO"><span class="nav-number">14.</span> <span class="nav-text">TAO</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#TAO-Toolkit-API"><span class="nav-number">14.1.</span> <span class="nav-text">TAO Toolkit API</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CV"><span class="nav-number">14.2.</span> <span class="nav-text">CV</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ClearML"><span class="nav-number">14.3.</span> <span class="nav-text">ClearML</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Deploy"><span class="nav-number">14.4.</span> <span class="nav-text">Deploy</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ClearML-2"><span class="nav-number">15.</span> <span class="nav-text">ClearML</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#pytorch"><span class="nav-number">16.</span> <span class="nav-text">pytorch</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Model-construct-strategy"><span class="nav-number">16.1.</span> <span class="nav-text">Model construct strategy</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Common-API"><span class="nav-number">16.2.</span> <span class="nav-text">Common API</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#tensor-equal"><span class="nav-number">16.2.1.</span> <span class="nav-text">tensor.equal</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#tensor-view"><span class="nav-number">16.2.2.</span> <span class="nav-text">tensor.view</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#tensor-reshape"><span class="nav-number">16.2.3.</span> <span class="nav-text">tensor.reshape</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#tensor-permute"><span class="nav-number">16.2.4.</span> <span class="nav-text">tensor.permute</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#tensor-transpose"><span class="nav-number">16.2.5.</span> <span class="nav-text">tensor.transpose</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#tensor-topk"><span class="nav-number">16.2.6.</span> <span class="nav-text">tensor.topk</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#tensor-softmax"><span class="nav-number">16.2.7.</span> <span class="nav-text">tensor.softmax</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#torch-no-gard"><span class="nav-number">16.2.8.</span> <span class="nav-text">torch.no_gard</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#torch-matmul"><span class="nav-number">16.2.9.</span> <span class="nav-text">torch.matmul</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#nn-Linear"><span class="nav-number">16.2.10.</span> <span class="nav-text">nn.Linear</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#nn-Dropout"><span class="nav-number">16.2.11.</span> <span class="nav-text">nn.Dropout</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#nn-Embedding"><span class="nav-number">16.2.12.</span> <span class="nav-text">nn.Embedding</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#nn-modules-loss-CrossEntropyLoss"><span class="nav-number">16.2.13.</span> <span class="nav-text">nn.modules.loss.CrossEntropyLoss</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#tensorflow"><span class="nav-number">17.</span> <span class="nav-text">tensorflow</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Model-construct-strategy-2"><span class="nav-number">17.1.</span> <span class="nav-text">Model construct strategy</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Common-api"><span class="nav-number">17.2.</span> <span class="nav-text">Common api</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#tf-trainable-variables"><span class="nav-number">17.2.1.</span> <span class="nav-text">tf.trainable_variables()</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Adam"><span class="nav-number">17.3.</span> <span class="nav-text">Adam</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Tips-2"><span class="nav-number">17.4.</span> <span class="nav-text">Tips</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#adam-m-and-adam-v"><span class="nav-number">17.4.1.</span> <span class="nav-text">adam_m and adam_v</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#display-with-tensorboard"><span class="nav-number">17.4.2.</span> <span class="nav-text">display with tensorboard</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#HuggingFace"><span class="nav-number">18.</span> <span class="nav-text">HuggingFace</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Transformers"><span class="nav-number">18.1.</span> <span class="nav-text">Transformers</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#export-to-ONNX"><span class="nav-number">18.1.1.</span> <span class="nav-text">export to ONNX</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#mlflow"><span class="nav-number">19.</span> <span class="nav-text">mlflow</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#mlflow-tracking"><span class="nav-number">19.1.</span> <span class="nav-text">mlflow tracking</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E4%BF%9D%E5%AD%98%E5%9C%A8%E6%9C%AC%E5%9C%B0"><span class="nav-number">19.1.1.</span> <span class="nav-text">数据保存在本地</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E4%BF%9D%E5%AD%98%E5%9C%A8database%E5%8F%8Aminio"><span class="nav-number">19.1.2.</span> <span class="nav-text">数据保存在 database 及 minio</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#mlflow-registry"><span class="nav-number">19.2.</span> <span class="nav-text">mlflow registry</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#mlflow-workflow"><span class="nav-number">19.3.</span> <span class="nav-text">mlflow workflow</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#mlflow-with-transformers"><span class="nav-number">19.4.</span> <span class="nav-text">mlflow with transformers</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#bert-sequence-classification"><span class="nav-number">19.4.1.</span> <span class="nav-text">bert sequence classification</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#mlflow-plugins"><span class="nav-number">19.5.</span> <span class="nav-text">mlflow plugins</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#MLServer"><span class="nav-number">20.</span> <span class="nav-text">MLServer</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#alibi"><span class="nav-number">21.</span> <span class="nav-text">alibi</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#DeepStream"><span class="nav-number">22.</span> <span class="nav-text">DeepStream</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Tritonserver"><span class="nav-number">23.</span> <span class="nav-text">Tritonserver</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Tutorials"><span class="nav-number">23.1.</span> <span class="nav-text">Tutorials</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Deploying-Using-Triton"><span class="nav-number">23.2.</span> <span class="nav-text">Deploying Using Triton</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Triton-Configuration"><span class="nav-number">23.3.</span> <span class="nav-text">Triton Configuration</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Model-Repository"><span class="nav-number">23.4.</span> <span class="nav-text">Model Repository</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Jetson-and-JetPack-Support"><span class="nav-number">23.5.</span> <span class="nav-text">Jetson and JetPack Support</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#HTTP-and-GRPC-Protocol-Support"><span class="nav-number">23.6.</span> <span class="nav-text">HTTP and GRPC Protocol Support</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Optimization"><span class="nav-number">23.7.</span> <span class="nav-text">Optimization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Backend"><span class="nav-number">23.8.</span> <span class="nav-text">Backend</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Tips-3"><span class="nav-number">23.9.</span> <span class="nav-text">Tips</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Model-format"><span class="nav-number">23.9.1.</span> <span class="nav-text">Model format</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Triton-REST-API"><span class="nav-number">23.9.2.</span> <span class="nav-text">Triton REST API</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9F%A5%E7%9C%8B%E6%A8%A1%E5%9E%8B%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA"><span class="nav-number">23.9.3.</span> <span class="nav-text">查看模型输入输出</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Yolov5-pt-to-ONNX"><span class="nav-number">23.9.4.</span> <span class="nav-text">Yolov5 pt to ONNX</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ALBERT-to-ONNX"><span class="nav-number">23.9.5.</span> <span class="nav-text">ALBERT to ONNX</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Transformers-to-ONNX"><span class="nav-number">23.9.6.</span> <span class="nav-text">Transformers to ONNX</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ONNX-Patch"><span class="nav-number">23.9.7.</span> <span class="nav-text">ONNX Patch</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ONNX-%E8%BD%AC%E6%8D%A2-TensorRT-Engine"><span class="nav-number">23.9.8.</span> <span class="nav-text">ONNX 转换 TensorRT Engine</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ONNX-TensorRT-Batch"><span class="nav-number">23.9.9.</span> <span class="nav-text">ONNX&#x2F;TensorRT Batch</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#dali-backend"><span class="nav-number">23.9.10.</span> <span class="nav-text">dali_backend</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8C%87%E5%AE%9Amodel%E6%8E%A8%E7%90%86"><span class="nav-number">23.9.11.</span> <span class="nav-text">指定 model 推理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Ensemble-pipeline"><span class="nav-number">23.9.12.</span> <span class="nav-text">Ensemble(pipeline)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Model-Warm-up"><span class="nav-number">23.9.13.</span> <span class="nav-text">Model Warm-up</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Jetson-Compatibility"><span class="nav-number">23.9.14.</span> <span class="nav-text">Jetson Compatibility</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#pytorch-with-triton"><span class="nav-number">23.9.15.</span> <span class="nav-text">pytorch with triton</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Solution"><span class="nav-number">23.10.</span> <span class="nav-text">Solution</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#k8s-Integration"><span class="nav-number">23.10.1.</span> <span class="nav-text">k8s Integration</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#mlflow-Integration"><span class="nav-number">23.10.2.</span> <span class="nav-text">mlflow Integration</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Customized-Server"><span class="nav-number">23.10.3.</span> <span class="nav-text">Customized Server</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#TensorRT"><span class="nav-number">24.</span> <span class="nav-text">TensorRT</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Tips-4"><span class="nav-number">24.1.</span> <span class="nav-text">Tips</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%89%E8%A3%85-2"><span class="nav-number">24.1.1.</span> <span class="nav-text">安装</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9F%A5%E7%9C%8B-tensorrt-%E7%89%88%E6%9C%AC"><span class="nav-number">24.1.2.</span> <span class="nav-text">查看 tensorrt 版本</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#HPC"><span class="nav-number">25.</span> <span class="nav-text">HPC</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Cuda"><span class="nav-number">26.</span> <span class="nav-text">Cuda</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#OpenVINO"><span class="nav-number">27.</span> <span class="nav-text">OpenVINO</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#DALI"><span class="nav-number">28.</span> <span class="nav-text">DALI</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#DALI-Operation"><span class="nav-number">28.1.</span> <span class="nav-text">DALI Operation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%89%E8%A3%85-3"><span class="nav-number">28.2.</span> <span class="nav-text">安装</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Common-Process"><span class="nav-number">28.3.</span> <span class="nav-text">Common Process</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%8E%B7%E5%8F%96%E5%BD%93%E5%89%8D%E7%AE%A1%E9%81%93%E4%BF%A1%E6%81%AF"><span class="nav-number">28.3.1.</span> <span class="nav-text">获取当前管道信息</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Image-Preprocess"><span class="nav-number">28.4.</span> <span class="nav-text">Image Preprocess</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Transformers-2"><span class="nav-number">29.</span> <span class="nav-text">Transformers</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#NLP"><span class="nav-number">30.</span> <span class="nav-text">NLP</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Yolov5"><span class="nav-number">31.</span> <span class="nav-text">Yolov5</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Resource"><span class="nav-number">31.1.</span> <span class="nav-text">Resource</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BE%93%E5%87%BAIOU"><span class="nav-number">31.2.</span> <span class="nav-text">输出 IOU</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#bert-albert"><span class="nav-number">32.</span> <span class="nav-text">bert&#x2F;albert</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E8%B5%84%E6%BA%90"><span class="nav-number">32.1.</span> <span class="nav-text">模型资源</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LLM"><span class="nav-number">33.</span> <span class="nav-text">LLM</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#ChatGLM"><span class="nav-number">33.1.</span> <span class="nav-text">ChatGLM</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B5%8B%E8%AF%95%E5%AE%98%E6%96%B9-Demo"><span class="nav-number">33.1.1.</span> <span class="nav-text">测试官方 Demo</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#VisualGLM"><span class="nav-number">33.2.</span> <span class="nav-text">VisualGLM</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ImageBind"><span class="nav-number">34.</span> <span class="nav-text">ImageBind</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#segment-anything"><span class="nav-number">35.</span> <span class="nav-text">segment-anything</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#milvus"><span class="nav-number">36.</span> <span class="nav-text">milvus</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Design"><span class="nav-number">36.1.</span> <span class="nav-text">Design</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9E%B6%E6%9E%84"><span class="nav-number">36.1.1.</span> <span class="nav-text">架构</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%B4%A2%E5%BC%95"><span class="nav-number">36.1.2.</span> <span class="nav-text">索引</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BA%A6%E9%87%8F"><span class="nav-number">36.1.3.</span> <span class="nav-text">度量</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Getting-started"><span class="nav-number">36.2.</span> <span class="nav-text">Getting started</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#install-2"><span class="nav-number">36.2.1.</span> <span class="nav-text">install</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Examples"><span class="nav-number">36.3.</span> <span class="nav-text">Examples</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Service-Encoding"><span class="nav-number">37.</span> <span class="nav-text">Service Encoding</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B0%86%E6%BA%90%E7%A0%81%E8%BD%AC%E4%B8%BAexe"><span class="nav-number">37.1.</span> <span class="nav-text">将源码转为 exe</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8E%88%E6%9D%83license"><span class="nav-number">37.2.</span> <span class="nav-text">授权 license</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8A%A0%E5%AF%86%E6%A8%A1%E5%9E%8B"><span class="nav-number">37.3.</span> <span class="nav-text">加密模型</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Msclock"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Msclock</p>
  <div class="site-description" itemprop="description">That life is reality.</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">50</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">32</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/msclock" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;msclock" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:msclock@qq.com" title="E-Mail → mailto:msclock@qq.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://plus.google.com/msclockg" title="Google → https:&#x2F;&#x2F;plus.google.com&#x2F;msclockg" rel="noopener me" target="_blank"><i class="fab fa-google fa-fw"></i>Google</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" class="cc-opacity" rel="noopener" target="_blank"><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="https://cdn.jsdelivr.net/npm/@creativecommons/vocabulary@2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://msclock.github.io/posts/c21efe28/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Msclock">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Live in the present">
      <meta itemprop="description" content="That life is reality.">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="AI 实践 | Live in the present">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          AI 实践
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-08-29 04:30:45" itemprop="dateCreated datePublished" datetime="2022-08-29T04:30:45+08:00">2022-08-29</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-04-17 01:13:34" itemprop="dateModified" datetime="2025-04-17T01:13:34+08:00">2025-04-17</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/develop/" itemprop="url" rel="index"><span itemprop="name">develop</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>59k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>54 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><!--
todo 待整理

https://www.zhihu.com/question/524022812/answer/2434021943
https://github.com/mli

## tensorflow

- https://tensorflow.google.cn/learn?hl=zh-cn
- https://tensorflow.google.cn/guide/gpu?hl=zh-cn

## ncnn

- https://zhuanlan.zhihu.com/p/449765328
- https://github.com/DefTruth/lite.ai.toolkit
- https://cloud.tencent.com/developer/salon/live-1346
- https://www.zhihu.com/question/441269200
- https://github.com/zchrissirhcz/awesome-ncnn

## 参考 AI 教程

[d2l](https://github.com/d2l-ai/d2l-zh)
[paper reading](https://github.com/mli/paper-reading)
[zhihu share](https://zhuanlan.zhihu.com/p/106712512)

## AI Paper With Code

- [CV PR 2023](https://github.com/amusi/CVPR2023-Papers-with-Code)

 -->
<span id="more"></span>
<h2 id="AI-资源">AI 资源</h2>
<hr>
<table>
<thead>
<tr>
<th>资源</th>
<th>组织</th>
<th>类型 </th>
</tr>
</thead>
<tbody>
<tr>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/">Hugging Face</a></td>
<td>Hugging Face</td>
<td>Community/Model</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://hub.docker.com/r/nvidia/cuda">Cuda Image Hub</a></td>
<td>Nvidia</td>
<td>Image/Cuda</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/overview.html">Container Toolkit</a></td>
<td>Nvidia</td>
<td>Runtime/Dev</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://www.mage.space/">mega.space</a></td>
<td>Mega</td>
<td>AI Image Generator</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://drive.google.com/">Colab</a></td>
<td>Google</td>
<td>Tools</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://image.delivery/">Image Delivery</a></td>
<td>image.delivery</td>
<td>Images(Stable Diffusions)</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://www.tutorialspoint.com/machine_learning_tutorials.htm">dl tutorialspoint</a></td>
<td>tutorialspoint</td>
<td>tutorial</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://agentgpt.reworkd.ai/zh">agentgpt</a></td>
<td>AgentGPT</td>
<td>Application</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://github.com/EwingYangs/awesome-open-gpt">awesome-open-gpt resource</a></td>
<td>EwingYangs</td>
<td>GPT  resource</td>
</tr>
</tbody>
</table>
<h2 id="Beginner">Beginner</h2>
<hr>
<!-- https://www.youtube.com/watch?v=wm9yR1VspPs
https://www.bilibili.com/video/BV1m3411p7wD?p=1
https://www.bilibili.com/video/BV16r4y1Y7jv

 -->
<h3 id="机器如何学习">机器如何学习</h3>
<p>从数据中找出规则（通过数学和程序）。</p>
<p>例如，通过图片识别出狗还是猫，只要足够的数据给到模型生成规则，那么再遇到类似的问题，就能够得到相对的答案，当正确率达到一定程度后，就代码这个模型相对稳定了。</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>模型，由各种数学和方程组成的规则构成。</p>
</li>
<li class="lvl-2">
<p>相对正确的答案，达到一定正确率。</p>
</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://drive.google.com/drive/folders/1_XZqvbhH7P0UbAH-epk-ThkV9Y6LtQwp">Colab 实验</a>.</p>
<h3 id="Colab">Colab</h3>
<p>谷歌提供的免费 <a target="_blank" rel="noopener" href="https://drive.google.com/">ML</a> 验证工具。</p>
<h3 id="ipynb-notebook">ipynb notebook</h3>
<h4 id="转为文件">转为文件</h4>
<ul class="lvl-0">
<li class="lvl-2">
<p>type: 转为的类型，包括 html、markdown、pdf、rst</p>
</li>
</ul>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jupyter nbconvert --to <span class="built_in">type</span> filename/as/ipynb</span><br></pre></td></tr></tbody></table></figure>
<h3 id="Math-in-AI">Math in AI</h3>
<!-- https://blog.csdn.net/hy592070616/article/details/81707766 -->
<h3 id="激活函数">激活函数</h3>
<!-- https://blog.csdn.net/hy592070616/article/details/120617176 -->
<!-- https://blog.csdn.net/leo_xu06/article/details/53708647 -->
<h4 id="sigmoid">sigmoid</h4>
<!-- https://zhuanlan.zhihu.com/p/424858561 -->
<h3 id="Palm-Detection">Palm Detection</h3>
<!-- https://www.youtube.com/watch?v=x4eeX7WJIuA -->
<h3 id="NHWC-to-NCHW">NHWC to NCHW</h3>
<p>在深度学习中，NHWC 和 NCHW 是两种常见的张量格式。NHWC 表示通道维度在最后一个维度，即 (batchsize, height, width, channels)，而 NCHW 表示通道维度在第二个维度，即 (batchsize, channels, height, width)。</p>
<p>将输入形状从 HWC 格式转换为 CHW 格式。具体来说，将输入形状从 (height, width, channels) 转换为 (channels, height, width)。这是因为在 PyTorch 中，张量的默认格式是 NCHW，而在 TensorFlow 中，张量的默认格式是 NHWC。因此，将输入形状从 HWC 格式转换为 CHW 格式可以使代码更易于在 PyTorch 中使用。</p>
<p>要将张量从 NHWC 格式转换为 NCHW 格式，可以使用以下代码：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert from NHWC to NCHW with torch.Tensor.permute</span></span><br><span class="line">tensor_nchw = tensor_nhwc.permute(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br></pre></td></tr></tbody></table></figure>
<p>在这里，tensor_nhwc 是一个形状为 (batch_size, height, width, channels) 的张量。tensor_nchw 是一个形状为 (batch_size, channels, height, width) 的张量，其中通道维度在第二个维度。tensor_nchw.permute (0, 3, 1, 2) 将张量的维度重新排列，以将通道维度移动到第二个维度。</p>
<h2 id="ml-in-c">ml in c</h2>
<p>links:</p>
<ul class="lvl-0">
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=PGSba51aRYU">https://www.youtube.com/watch?v=PGSba51aRYU</a></p>
</li>
</ul>
<h2 id="AI-copilot">AI copilot</h2>
<h3 id="cursor">cursor</h3>
<h3 id="connect-to-wsl">connect to wsl</h3>
<p>links:</p>
<ul class="lvl-0">
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://github.com/getcursor/cursor/issues/660#issuecomment-1678308600">https://github.com/getcursor/cursor/issues/660#issuecomment-1678308600</a></p>
</li>
</ul>
<h3 id="cocopilot">cocopilot</h3>
<p>patch the code copilot over the common IDE.</p>
<p>links:</p>
<ul class="lvl-0">
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://github.com/pengzhile/cocopilot">https://github.com/pengzhile/cocopilot</a></p>
</li>
</ul>
<h3 id="open-interpreter">open-interpreter</h3>
<p>links:</p>
<ul class="lvl-0">
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://github.com/KillianLucas/open-interpreter">https://github.com/KillianLucas/open-interpreter</a></p>
</li>
</ul>
<h2 id="numpy">numpy</h2>
<hr>
<p><a target="_blank" rel="noopener" href="https://numpy.org/doc/stable/index.html">numpy</a> 是一个 Python 库，用于处理多维数组和矩阵，以及用于数学计算的函数集合。它是 Python 科学计算的核心库之一，因为它提供了高效的数据结构和计算工具，使得 Python 成为了一种流行的科学计算语言。</p>
<h3 id="Tips">Tips</h3>
<h4 id="增删维度">增删维度</h4>
<p>图片数据经常 batch 处理，此时的维度为 <code>[batch, c, h, w]</code> 或 <code>[batch, h, w, c]</code>，为适配输入输出，需要增加和删除 batch</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">image_origin = np.from_file(file, np.uint8)</span><br><span class="line"><span class="comment"># 增加维度</span></span><br><span class="line">with_batch = np.expand_dims(image_origin, axis=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 删除维度</span></span><br><span class="line">without_batch = np.squeeze(with_batch, axis=<span class="number">0</span>)</span><br></pre></td></tr></tbody></table></figure>
<h3 id="NHWC-to-NCHW-2">NHWC to NCHW</h3>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">padding_img = padding_img.transpose((<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>))[::-<span class="number">1</span>]  <span class="comment"># HWC to CHW, BGR to RGB</span></span><br><span class="line">padding_img = np.ascontiguousarray(padding_img)</span><br></pre></td></tr></tbody></table></figure>
<h2 id="pandas">pandas</h2>
<h3 id="api">api</h3>
<h4 id="quantile">quantile</h4>
<p>使用 quantile 函数计算分数位数. quantile 计算的原理是通过对数据进行排序，找到给定百分位数位置的值。具体来说，对于一个已排序的数据集，quantile 函数会根据指定的百分位数来确定在该百分位数位置处的值。</p>
<p>例如，对于一个包含 100 个数据点的数据集，如果想要计算 p99（百分之 99 的数据点的值）即 <code>DataFrame.quantile(0.99)</code>，需要找到位于排好序的数据集中第 99 个百分位数位置的值。</p>
<p>quantile 函数的计算方式可以有多种方法，其中一种常用的方法是线性插值法。该方法首先计算出百分位数位置的整数部分索引和小数部分的权重，并根据权重对相邻的数据点进行插值计算，以得出最终的百分位数值。</p>
<h2 id="ONNX">ONNX</h2>
<ul class="lvl-0">
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://onnx.ai/onnx/index.html">ONNX DOC</a></p>
</li>
</ul>
<h2 id="OpenCV">OpenCV</h2>
<hr>
<!-- https://www.youtube.com/watch?v=xjrykYpaBBM -->
<h3 id="安装">安装</h3>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">apt-get update &amp;&amp; apt-get install -y --no-install-recommends libgl1 &amp;&amp; pip install opencv-python opencv-python-headless</span><br></pre></td></tr></tbody></table></figure>
<blockquote>
<p>[!TIP]<br>
<a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/55313610/importerror-libgl-so-1-cannot-open-shared-object-file-no-such-file-or-directo">Error</a>: ImportError: libGL.so.1: cannot open shared object file: No such file or directory.</p>
<p>Solution: <code>pip install opencv-python opencv-python-headless</code>.</p>
</blockquote>
<h3 id="Tip">Tip</h3>
<h4 id="jpg-to-numpy">jpg to numpy</h4>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">from</span> Pillow <span class="keyword">import</span> PIL  <span class="comment"># need install pillow</span></span><br><span class="line"></span><br><span class="line">raw_img_np = np.fromfile(file_path, dtype=<span class="string">"uint8"</span>)</span><br><span class="line"><span class="comment"># convert from Pillow.Image.open</span></span><br><span class="line">image_p = PIL.Image.<span class="built_in">open</span>(io.BytesIO(raw_img_np.tobytes()))</span><br><span class="line">tensor = np.array(image_p)</span><br><span class="line"><span class="comment"># convert from cv2.imdecode</span></span><br><span class="line">tensor = cv2.imdecode(raw_img_np, cv2.IMREAD_COLOR)</span><br><span class="line"><span class="comment"># read from cv2.imread</span></span><br><span class="line">tensor = cv2.imread(file_path)</span><br></pre></td></tr></tbody></table></figure>
<h2 id="GPT">GPT</h2>
<h3 id="资源">资源</h3>
<table>
<thead>
<tr>
<th>资源</th>
<th>组织</th>
<th>类型</th>
</tr>
</thead>
<tbody>
<tr>
<td><a target="_blank" rel="noopener" href="https://www.yuque.com/if">西鲸 AI 智脑</a></td>
<td>西鲸</td>
<td>文档 </td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://chat.dfehub.com/">chat.dfehub</a></td>
<td> 西鲸</td>
<td> rest api</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://github.com/ramonvc/freegpt-webui">freegpt-webui</a></td>
<td>ramonvc</td>
<td>freegpt web app</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://github.com/h2oai/h2ogpt">h2ogpt</a></td>
<td>h2oai</td>
<td>private gpt server</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://github.com/StanGirard/quivr">quivr</a></td>
<td>StanGirard</td>
<td>AI Brain</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://github.com/zurawiki/gptcommit">gitcommit</a></td>
<td>zurawiki</td>
<td>git commit tools</td>
</tr>
</tbody>
</table>
<h3 id="实用工具">实用工具</h3>
<p>浏览器 Edge/Chrome</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>ChatGPT Sidebar</p>
</li>
</ul>
<p>注册工具</p>
<ul class="lvl-0">
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://sms-activate.org/">SMS</a></p>
</li>
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://www.zoho.com/mail/">zohomail</a></p>
</li>
</ul>
<h3 id="Stable-Diffusion">Stable Diffusion</h3>
<p>AI 文字转图片工具<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=147uG9Heotk">使用教程</a>.</p>
<p>Stable Diffusion <a href="civitai.com">AI models 下载</a>。这里有推荐的<a target="_blank" rel="noopener" href="https://www.freedidi.com/8621.html">模型</a>及软件下载<a target="_blank" rel="noopener" href="https://www.freedidi.com/8474.html">链接</a>.</p>
<h3 id="gptcommit">gptcommit</h3>
<p><a target="_blank" rel="noopener" href="https://github.com/zurawiki/gptcommit">gptcommit</a> 自动根据代码更改生成 commit 消息.</p>
<blockquote>
<p>[!TIP]<br>
可以直接使用 vscode 插件集成 vscode-gptcommit</p>
</blockquote>
<h3 id="prompt">prompt</h3>
<p>随着 ChatGPT 带来的 AI 大模型的技术突破，也带来了一项新的 AI 工程技术，即 prompt (用于规则化提问，生成预计业务逻辑回答). 这项技术让 AI 从对话中的生成回答能够真正地落地于其它应用工程。比如前面提到的 gptcommit, gpt-migrate 等都是对 prompt 的应用.</p>
<blockquote>
<p>[!NOTE]<br>
prompt: 简单来说就是将问题模板化，进而生成预期结果.</p>
</blockquote>
<p>示例:</p>
<ul class="lvl-0">
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://github.com/StanGirard/quivr/blob/fix/preview/backend/llm/prompt/LANGUAGE_PROMPT.py">https://github.com/StanGirard/quivr/blob/fix/preview/backend/llm/prompt/LANGUAGE_PROMPT.py</a></p>
</li>
</ul>
<h2 id="AI-Workflow">AI Workflow</h2>
<p>通常的 AI 工作流如下：</p>
<pre><code class="highlight mermaid">graph LR
    subgraph ETL/ML Engineer
        AI-Train[AI Training] --&gt;|Trained Models| Model-Optimization[Model-Optimization]
        Model-Optimization --&gt;|Optimized Models| Model-Storage[(Model-Storage)]
    end
    subgraph Dev/ML Ops
        Model-Storage --&gt;|Deploy| Inference-Serving[Inference-Serving]
    end
    subgraph App Developer
        Inference-Serving &lt;--&gt;|Query/Result| Application[Application]
    end</code></pre>
<h2 id="AI-Serving">AI Serving</h2>
<p>服务化的需求：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>可扩展 pytorch、tensorflow、onnx 等类型模型的推理。</p>
</li>
<li class="lvl-2">
<p>能够支持 http grpc 通信。</p>
</li>
<li class="lvl-2">
<p>基于 pipline 处理流程，方便扩展流水线。</p>
</li>
<li class="lvl-2">
<p>能够支持高并发处理。</p>
</li>
<li class="lvl-2">
<p>可参考 triton 服务，调研其他框架。</p>
</li>
</ul>
<p>理想方案</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>mode inference -&gt; kserve {http/grpc}  + [ 业务代码服务 { flask } + [ dapr ] ]。</p>
</li>
<li class="lvl-2">
<p>业务代码 inference -&gt; kserve {http/grpc} 。</p>
</li>
</ul>
<p>现有方案：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>多模型同一个容器运行。</p>
</li>
<li class="lvl-2">
<p>提供 http 请求。</p>
</li>
</ul>
<p>其它方案 demo 或技术：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://github.com/wmeints/mlops-airflow-sample">mlops-airflow-sample</a></p>
</li>
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/569409465">ml stack</a></p>
</li>
</ul>
<h3 id="AI-Serving-Solution">AI Serving Solution</h3>
<h4 id="API服务工具">API 服务工具</h4>
<p>Torch Serving、Tensor Serving 和 Triton Serving 都是用于将机器学习模型部署为 API 服务的工具。它们都提供了类似的功能，例如模型加载、推理请求处理和模型版本控制等。</p>
<p>然而，它们之间也存在一些区别。以下是一些了解到的区别：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>Torch Serving 是由 PyTorch 社区开发的，专门用于部署 PyTorch 模型。它支持多种模型格式，例如 TorchScript 和 ONNX 等。</p>
</li>
<li class="lvl-2">
<p>Tensor Serving 是由 Google 开发的，专门用于部署 TensorFlow 模型。它支持 TensorFlow SavedModel 格式。</p>
</li>
<li class="lvl-2">
<p>Triton Serving 是由 NVIDIA 开发的，专门用于部署深度学习模型。它支持多种模型格式，例如 TensorFlow、PyTorch 和 ONNX 等。</p>
</li>
<li class="lvl-2">
<p>ONNX Runtime：ONNX Runtime 是一个用于部署 ONNX 模型的高性能、跨平台的推理引擎。它支持多种硬件平台和操作系统，包括 CPU、GPU、FPGA 和 Edge 设备等。可以使用 ONNX Runtime 来加载 ONNX 模型并提供服务，同时还可以使用其提供的 API 来管理模型版本、优化推理性能等。</p>
</li>
</ul>
<p>因此，虽然这些工具都提供了类似的功能，但它们的实现方式和支持的模型格式可能会有所不同。</p>
<h4 id="AI-Dev-辅助框架">AI Dev 辅助框架</h4>
<ul class="lvl-0">
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://airflow.apache.org/docs/apache-airflow/stable/">Airflow</a>: 用于使用编程的方式实现开发、调度和监控面向批处理的工作流。</p>
<ul class="lvl-2">
<li class="lvl-4"><a target="_blank" rel="noopener" href="https://github.com/marclamberti/airflow-kubernetes">airflow 集成到 k8s</a>。</li>
</ul>
</li>
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://docs.dapr.io/">Dapr</a>: Dapr 是一个开源的分布式应用程序运行时，它提供了一组构建块，用于简化应用程序的开发和部署。其中包括 Service Invocation、State Management、Pub/Sub 等功能，可以帮助快速构建和部署 AI 服务。Dapr 还提供了多种通信协议和 SDK，方便与其他服务进行交互。</p>
</li>
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://www.kubeflow.org/">Kubeflow</a>: Kubeflow 是一个用于部署机器学习工作流的开源平台。它基于 Kubernetes 构建，提供了一组工具和 API，用于管理模型训练、推理、部署等过程。可以使用 Kubeflow 来构建端到端的机器学习工作流，同时还可以使用其提供的组件来部署和管理 AI 服务。</p>
<ul class="lvl-2">
<li class="lvl-4"><a target="_blank" rel="noopener" href="https://github.com/alauda/kubeflow-chart">国内扩展 alauda</a>。</li>
<li class="lvl-4"><a target="_blank" rel="noopener" href="https://github.com/tencentmusic/cube-studio">国内相似平台 cubestudio</a>。</li>
</ul>
</li>
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://github.com/kserve">kServe</a>: 是一个用于构建和部署机器学习模型的开源框架。它提供了一个简单易用的 API，可以帮助快速构建和部署机器学习模型。kServe 支持多种模型格式，包括 TensorFlow、PyTorch、ONNX 等。它还提供了多种部署方式，例如 HTTP、gRPC 等，可以满足不同场景下的需求。以下是一些有用的链接：</p>
<ul class="lvl-2">
<li class="lvl-4"><a target="_blank" rel="noopener" href="https://kserve.github.io/website/">kServe 官方文档</a></li>
<li class="lvl-4"><a target="_blank" rel="noopener" href="https://github.com/kserve/kserve">kServe GitHub 仓库</a></li>
<li class="lvl-4"><a target="_blank" rel="noopener" href="https://kserve.github.io/website/0.10/get_started/first_isvc/">kServe inferenceService</a></li>
</ul>
</li>
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://developer.nvidia.com/tao-toolkit">TAO Toolkit</a>：是一个用于训练和部署深度学习模型的工具包，它提供了一系列的工具和库，可以帮助用户快速构建和部署深度学习模型。在使用 TAO Toolkit 之前，需要先了解深度学习的基本概念和使用方法。可以参考以下文档：</p>
<ul class="lvl-2">
<li class="lvl-4"><a target="_blank" rel="noopener" href="https://deeplearning4j.org/cn/index.html">Deep Learning 中文文档</a></li>
<li class="lvl-4"><a target="_blank" rel="noopener" href="https://www.tensorflow.org/">TensorFlow 官方文档</a></li>
<li class="lvl-4"><a target="_blank" rel="noopener" href="https://pytorch.org/">PyTorch 官方文档</a></li>
<li class="lvl-4"><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/index.html">NVIDIA GPU 官方文档</a></li>
<li class="lvl-4"><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html">CUDA 官方文档</a></li>
</ul>
</li>
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://github.com/modzy/chassis">Chassis</a>: 是一个开源项目，可以在几分钟内将 ML 模型转换为容器化的预测 API。</p>
<ul class="lvl-2">
<li class="lvl-4"><a target="_blank" rel="noopener" href="https://chassis.ml/tutorials/ds-deploy/">Chassis</a> 脚本化部署。</li>
<li class="lvl-4"><a target="_blank" rel="noopener" href="https://chassis.ml/getting-started/getting-started/">Chassis Getting Started</a>。</li>
</ul>
</li>
</ul>
<h3 id="TensorFlow-Serving">TensorFlow Serving</h3>
<h4 id="能否给一个使用-TensorFlow-Serving-将机器学习模型部署为服务的示例">能否给一个使用 TensorFlow Serving 将机器学习模型部署为服务的示例</h4>
<p>想要了解如何使用 TensorFlow Serving 将机器学习模型部署为服务。以下是一个使用 TensorFlow Serving 调用 TensorFlow 模型的示例：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>首先，需要安装 TensorFlow Serving。可以使用以下命令在 Linux 上安装 TensorFlow Serving：</p>
</li>
</ul>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install tensorflow-serving-api</span><br></pre></td></tr></tbody></table></figure>
<ul class="lvl-0">
<li class="lvl-2">
<p>然后，需要启动 TensorFlow Serving。可以使用以下命令启动 TensorFlow Serving：</p>
</li>
</ul>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensorflow_model_server --port=8500 --rest_api_port=8501 --model_name=my_model --model_base_path=path/to/your/model</span><br></pre></td></tr></tbody></table></figure>
<p>其中，my_model 是模型的名称，path/to/your/model 是模型存储路径。</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>接下来，需要使用 TensorFlow Client 调用模型。以下是一个使用 TensorFlow Client 调用模型的示例：</p>
</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义输入数据</span></span><br><span class="line">input_data = {<span class="string">"instances"</span>: [{<span class="string">"input"</span>: np.random.rand(<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>).tolist()}]}</span><br><span class="line"></span><br><span class="line"><span class="comment"># 发送请求</span></span><br><span class="line">response = requests.post(</span><br><span class="line">    <span class="string">"http://localhost:8501/v1/models/my_model:predict"</span>, json=input_data</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 解析响应</span></span><br><span class="line">output_data = json.loads(response.content.decode(<span class="string">"utf-8"</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取输出张量</span></span><br><span class="line">output_tensor = np.array(output_data[<span class="string">"predictions"</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 处理输出张量</span></span><br></pre></td></tr></tbody></table></figure>
<p>其中，my_model 是模型的名称。</p>
<h3 id="Torch-Serving">Torch Serving</h3>
<h4 id="能否给一个使用-Torch-Serving-将机器学习模型部署为服务的示例">能否给一个使用 Torch Serving 将机器学习模型部署为服务的示例</h4>
<p>想要了解如何使用 Torch Serving 将机器学习模型部署为服务。以下是一个使用 Torch Serving 部署 PyTorch 模型的示例：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>首先，需要安装 Torch Serving。可以使用以下命令在 Linux 上安装 Torch Serving：</p>
</li>
</ul>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install torchserve torch-model-archiver</span><br></pre></td></tr></tbody></table></figure>
<ul class="lvl-0">
<li class="lvl-2">
<p>接下来，需要将 PyTorch 模型转换为 TorchScript 格式。可以使用以下代码将 PyTorch 模型转换为 TorchScript：</p>
</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载 PyTorch 模型</span></span><br><span class="line">model = torch.load(<span class="string">"path/to/your/model.pth"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将模型转换为 TorchScript</span></span><br><span class="line">example_input = torch.rand(<span class="number">1</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>)</span><br><span class="line">traced_script_module = torch.jit.trace(model, example_input)</span><br><span class="line">traced_script_module.save(<span class="string">"path/to/your/model.pt"</span>)</span><br></pre></td></tr></tbody></table></figure>
<ul class="lvl-0">
<li class="lvl-2">
<p>然后，需要创建一个模型描述文件。该文件描述了模型的输入和输出格式。以下是一个示例模型描述文件：</p>
</li>
</ul>
<figure class="highlight json"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">{</span></span><br><span class="line">  <span class="attr">"input_shape"</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="number">1</span><span class="punctuation">,</span> <span class="number">3</span><span class="punctuation">,</span> <span class="number">224</span><span class="punctuation">,</span> <span class="number">224</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">"output_shape"</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="number">1</span><span class="punctuation">,</span> <span class="number">1000</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">"mean"</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="number">0.485</span><span class="punctuation">,</span> <span class="number">0.456</span><span class="punctuation">,</span> <span class="number">0.406</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">"std"</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="number">0.229</span><span class="punctuation">,</span> <span class="number">0.224</span><span class="punctuation">,</span> <span class="number">0.225</span><span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">}</span></span><br></pre></td></tr></tbody></table></figure>
<ul class="lvl-0">
<li class="lvl-2">
<p>接下来，需要使用 Torch Model Archiver 将模型打包为 Torch Serving 可以使用的格式。可以使用以下命令将模型打包：</p>
</li>
</ul>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch-model-archiver --model-name my_model --version 1.0 --serialized-file path/to/your/model.pt --handler path/to/your/handler.py --extra-files path/to/your/model_description.json --export-path model_store</span><br></pre></td></tr></tbody></table></figure>
<p>其中，my_model 是模型的名称，1.0 是模型的版本号，path/to/your/model.pt 是 TorchScript 模型的路径，path/to/your/handler.py 是模型处理程序的路径，path/to/your/model_description.json 是模型描述文件的路径，model_store 是要导出模型的路径。</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>最后，可以使用以下命令启动 Torch Serving：</p>
</li>
</ul>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torchserve --start --model-store model_store --models my_model=your_model.mar</span><br></pre></td></tr></tbody></table></figure>
<p>其中，model_store 是模型存储路径，my_model 是模型的名称，your_model.mar 是模型的 Torch Serving 格式的文件。</p>
<p>这就是使用 Torch Serving 将 PyTorch 模型部署为服务的示例。</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>接下来，需要使用 Torch Client 调用模型。以下是一个使用 Torch Client 调用模型的示例：</p>
</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> base64</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义输入数据</span></span><br><span class="line">input_data = {</span><br><span class="line">    <span class="string">"data"</span>: base64.b64encode(<span class="built_in">open</span>(<span class="string">"path/to/your/image.jpg"</span>, <span class="string">"rb"</span>).read()).decode(</span><br><span class="line">        <span class="string">"utf-8"</span></span><br><span class="line">    ),</span><br><span class="line">    <span class="string">"content_type"</span>: <span class="string">"application/octet-stream"</span>,</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment"># 发送请求</span></span><br><span class="line">response = requests.post(<span class="string">"http://localhost:8080/predictions/my_model"</span>, json=input_data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 解析响应</span></span><br><span class="line">output_data = json.loads(response.content.decode(<span class="string">"utf-8"</span>))</span><br></pre></td></tr></tbody></table></figure>
<p>其中，path/to/your/image.jpg 是输入图像的路径，my_model 是模型的名称。</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>最后，可以使用 output_data 变量中的结果。结果的格式取决于模型的输出格式。例如，如果模型输出一个张量，可以使用以下代码获取该张量：</p>
</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取输出张量</span></span><br><span class="line">output_tensor = np.array(output_data[<span class="string">"predictions"</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 处理输出张量</span></span><br></pre></td></tr></tbody></table></figure>
<p>这就是使用 Torch Serving 调用 PyTorch 模型的示例。</p>
<h3 id="Triton-Serving">Triton Serving</h3>
<h4 id="如何使用Triton-Serving部署AI服务">如何使用 Triton Serving 部署 AI 服务</h4>
<p>为了使用 Triton Serving 部署 AI 服务，可以按照以下步骤进行操作：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>按照官方文档提供的说明安装 Triton Serving：<a target="_blank" rel="noopener" href="https://github.com/triton-inference-server/server/blob/main/docs/quickstart.md">https://github.com/triton-inference-server/server/blob/main/docs/quickstart.md</a></p>
</li>
<li class="lvl-2">
<p>将模型转换为 Triton Serving 可以理解的格式，以准备部署。Triton Serving 支持多种模型格式，包括 TensorFlow SavedModel、ONNX 和 PyTorch。可以在 Triton Serving 文档中找到有关如何准备模型的更多信息：<a target="_blank" rel="noopener" href="https://github.com/triton-inference-server/server/blob/main/docs/modelrepository.md">https://github.com/triton-inference-server/server/blob/main/docs/modelrepository.md</a></p>
</li>
<li class="lvl-2">
<p>准备好模型后，可以按照 Triton Serving 文档中的说明创建模型存储库：<a target="_blank" rel="noopener" href="https://github.com/triton-inference-server/server/blob/main/docs/modelrepository.md#creating-a-model-repository">https://github.com/triton-inference-server/server/blob/main/docs/modelrepository.md#creating-a-model-repository</a></p>
</li>
<li class="lvl-2">
<p>运行以下命令启动 Triton Serving 服务器：</p>
</li>
</ul>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tritonserver --model-repository=/path/to/model/repository</span><br></pre></td></tr></tbody></table></figure>
<p>将 /path/to/model/repository 替换为模型存储库的路径。</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>服务器运行后，可以使用 Triton Serving 客户端库将推理请求发送到服务器。Triton Serving 支持多种客户端库，包括 Python、C++ 和 Java。可以在 Triton Serving 文档中找到有关如何使用客户端库的更多信息：<a target="_blank" rel="noopener" href="https://github.com/triton-inference-server/client">https://github.com/triton-inference-server/client</a></p>
</li>
</ul>
<h3 id="Airflow">Airflow</h3>
<h4 id="如何用Airflow框架实现模型常驻在内存的AI服务">如何用 Airflow 框架实现模型常驻在内存的 AI 服务</h4>
<p>首先，Airflow 框架主要用于调度和监控数据处理任务，而不是用于实现 AI 服务。但是，可以使用 Airflow 来调度和监控运行 AI 服务的任务。</p>
<p>要实现模型常驻在内存的 AI 服务，需要使用一个 Web 框架（如 Flask 或 Django）来创建一个 API，该 API 将加载模型并接收输入数据以进行预测。然后，可以使用 Airflow 来调度和监控此 API 的运行。</p>
<p>以下是一些可能有用的步骤：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>使用 Flask 或 Django 创建一个 API，该 API 将加载模型并接收输入数据以进行预测。可以使用以下代码作为起点：</p>
</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> Flask, request</span><br><span class="line"></span><br><span class="line">app = Flask(__name__)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">"/predict"</span>, methods=[<span class="string">"POST"</span>]</span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">predict</span>():</span><br><span class="line">    ...</span><br><span class="line">    <span class="comment"># Load model</span></span><br><span class="line">    <span class="comment"># Get input data from request</span></span><br><span class="line">    <span class="comment"># Make prediction</span></span><br><span class="line">    <span class="comment"># Return prediction</span></span><br></pre></td></tr></tbody></table></figure>
<ul class="lvl-0">
<li class="lvl-2">
<p>将 API 部署到服务器上，并确保它可以在后台运行。可以使用 gunicorn 或 uwsgi 等工具来运行 API。</p>
</li>
<li class="lvl-2">
<p>创建一个 Airflow DAG，该 DAG 将定期调用 API 以确保其保持活动状态。可以使用以下代码作为起点：</p>
</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> airflow <span class="keyword">import</span> DAG</span><br><span class="line"><span class="keyword">from</span> airflow.operators.http_operator <span class="keyword">import</span> SimpleHttpOperator</span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime, timedelta</span><br><span class="line"></span><br><span class="line">default_args = {</span><br><span class="line">    <span class="string">"owner"</span>: <span class="string">"airflow"</span>,</span><br><span class="line">    <span class="string">"depends_on_past"</span>: <span class="literal">False</span>,</span><br><span class="line">    <span class="string">"start_date"</span>: datetime(<span class="number">2021</span>, <span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">    <span class="string">"retries"</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">"retry_delay"</span>: timedelta(minutes=<span class="number">5</span>),</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">dag = DAG(</span><br><span class="line">    <span class="string">"ai_service"</span>,</span><br><span class="line">    default_args=default_args,</span><br><span class="line">    schedule_interval=timedelta(minutes=<span class="number">30</span>),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">predict_task = SimpleHttpOperator(</span><br><span class="line">    task_id=<span class="string">"predict"</span>,</span><br><span class="line">    method=<span class="string">"POST"</span>,</span><br><span class="line">    http_conn_id=<span class="string">"my_api"</span>,</span><br><span class="line">    endpoint=<span class="string">"/predict"</span>,</span><br><span class="line">    headers={<span class="string">"Content-Type"</span>: <span class="string">"application/json"</span>},</span><br><span class="line">    data=<span class="string">'{"input": "data"}'</span>,</span><br><span class="line">    dag=dag,</span><br><span class="line">)</span><br></pre></td></tr></tbody></table></figure>
<ul class="lvl-0">
<li class="lvl-2">
<p>将 Airflow DAG 部署到 Airflow 服务器上，并确保它可以在后台运行。</p>
</li>
</ul>
<h4 id="Airflow能否手动配置运行这个AI服务的数量">Airflow 能否手动配置运行这个 AI 服务的数量</h4>
<p>Airflow 本身并不提供手动配置运行任务的数量的功能。但是，可以通过修改 DAG 的 scheduleinterval 参数来控制任务的运行频率，从而间接地控制任务的数量。例如，将 scheduleinterval 设置为 timedelta (minutes=1) 将使任务每分钟运行一次，从而增加任务的数量。</p>
<p>另外，可以使用 Airflow 的任务并发设置来控制同时运行的任务数量。可以在 Airflow 的配置文件中设置 maxactiverunsperdag 参数来限制每个 DAG 同时运行的任务数量。例如，将 maxactiverunsperdag 设置为 2 将使每个 DAG 最多同时运行 2 个任务。</p>
<p>请注意，这些设置可能会影响服务器资源使用情况，因此需要根据具体需求进行调整。</p>
<h3 id="Dapr">Dapr</h3>
<h4 id="如何用Dapr框架实现模型常驻在内存的AI服务">如何用 Dapr 框架实现模型常驻在内存的 AI 服务</h4>
<p>首先，Dapr 是一个开源的微服务框架，它可以帮助开发人员构建分布式应用程序。它提供了一些有用的功能，例如状态管理，消息传递和服务发现。但是，它本身并不是一个 AI 框架，因此需要使用其他 AI 框架来实现模型常驻在内存的 AI 服务。</p>
<p>一种常见的方法是使用 TensorFlow Serving。TensorFlow Serving 是一个用于生产环境的 TensorFlow 模型服务器，它可以轻松地将 TensorFlow 模型部署到生产环境中。可以使用 Dapr 框架来管理 TensorFlow Serving 实例，并使用 Dapr 的状态管理功能来存储模型的元数据和状态。</p>
<p>以下是一个使用 Dapr 和 TensorFlow Serving 实现模型常驻在内存的 AI 服务的示例：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>首先，需要安装 Dapr 和 TensorFlow Serving。可以按照官方文档进行安装。</p>
</li>
<li class="lvl-2">
<p>然后，需要编写一个 Dapr 组件来管理 TensorFlow Serving 实例。可以在 Dapr 配置文件中定义此组件，如下所示：</p>
</li>
</ul>
<figure class="highlight yml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">dapr.io/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Component</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">tensorflow-serving</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">external</span></span><br><span class="line">  <span class="attr">metadata:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">grpcPort</span></span><br><span class="line">      <span class="attr">value:</span> <span class="string">"8500"</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">restPort</span></span><br><span class="line">      <span class="attr">value:</span> <span class="string">"8501"</span></span><br><span class="line">  <span class="attr">version:</span> <span class="string">v1</span></span><br><span class="line">  <span class="attr">description:</span> <span class="string">"TensorFlow Serving component"</span></span><br><span class="line">  <span class="attr">status:</span></span><br><span class="line">    <span class="attr">address:</span> <span class="string">"localhost:8500"</span></span><br></pre></td></tr></tbody></table></figure>
<ul class="lvl-0">
<li class="lvl-2">
<p>接下来，需要编写一个 Dapr 应用程序来使用 TensorFlow Serving 组件。可以使用 Dapr 的状态管理功能来存储模型的元数据和状态。以下是一个使用 Python 编写的示例：</p>
</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> dapr.clients</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize Dapr client</span></span><br><span class="line"><span class="keyword">with</span> dapr.clients.DaprClient() <span class="keyword">as</span> d:</span><br><span class="line">    <span class="comment"># Load TensorFlow model</span></span><br><span class="line">    model = tf.keras.models.load_model(<span class="string">"path/to/model"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Register model with TensorFlow Serving</span></span><br><span class="line">    response = d.invoke_service(</span><br><span class="line">        <span class="string">"tensorflow-serving"</span>,</span><br><span class="line">        <span class="string">"v1/models/model:predict"</span>,</span><br><span class="line">        data=model.to_json(),</span><br><span class="line">        metadata={<span class="string">"content-type"</span>: <span class="string">"application/json"</span>},</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Store model metadata and state in Dapr state store</span></span><br><span class="line">    d.save_state(</span><br><span class="line">        <span class="string">"model-metadata"</span>,</span><br><span class="line">        {</span><br><span class="line">            <span class="string">"model_name"</span>: <span class="string">"model"</span>,</span><br><span class="line">            <span class="string">"model_version"</span>: <span class="string">"1"</span>,</span><br><span class="line">            <span class="string">"model_input_shape"</span>: model.input_shape,</span><br><span class="line">        },</span><br><span class="line">    )</span><br><span class="line">    d.save_state(<span class="string">"model-state"</span>, {<span class="string">"status"</span>: <span class="string">"running"</span>})</span><br></pre></td></tr></tbody></table></figure>
<p>在此示例中，首先使用 Dapr 客户端加载 TensorFlow 模型。然后，使用 Dapr 客户端调用 TensorFlow Serving 组件，并将模型注册到 TensorFlow Serving 中。最后，使用 Dapr 客户端将模型的元数据和状态存储在 Dapr 状态存储中。</p>
<p>已经使用 Dapr 客户端加载了 TensorFlow 模型，并将其注册到 TensorFlow Serving 中。要调用加载的模型，可以使用 Dapr 客户端调用 TensorFlow Serving 组件的 REST API。</p>
<p>以下是一个使用 Python 编写的示例：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> dapr.clients</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize Dapr client</span></span><br><span class="line"><span class="keyword">with</span> dapr.clients.DaprClient() <span class="keyword">as</span> d:</span><br><span class="line">    <span class="comment"># Call TensorFlow Serving REST API</span></span><br><span class="line">    response = d.invoke_binding(</span><br><span class="line">        <span class="string">"tensorflow-serving"</span>,</span><br><span class="line">        <span class="string">"rest"</span>,</span><br><span class="line">        data={<span class="string">"instances"</span>: [[<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>, <span class="number">4.0</span>]]},</span><br><span class="line">        metadata={<span class="string">"content-type"</span>: <span class="string">"application/json"</span>},</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Process response</span></span><br><span class="line">    <span class="built_in">print</span>(response.content)</span><br></pre></td></tr></tbody></table></figure>
<p>在此示例中，使用 Dapr 客户端调用 TensorFlow Serving 组件的 REST API，并将输入数据作为 JSON 格式的数据发送。可以根据自己的需求修改输入数据的格式和内容。最后，打印出 TensorFlow Serving 返回的响应。</p>
<h4 id="Dapr能否手动配置运行上面提到的AI服务数量">Dapr 能否手动配置运行上面提到的 AI 服务数量</h4>
<p>Dapr 是一个开源的分布式应用程序运行时，它可以帮助开发人员轻松地构建和运行微服务应用程序。Dapr 提供了一种简单的方式来注册和发现服务，并提供了一组 API 来调用这些服务。在使用 Dapr 和 TensorFlow Serving 实现模型常驻在内存的 AI 服务的基础上，可以使用 Dapr 客户端调用 TensorFlow Serving 组件的 REST API，或者将 TensorFlow Serving 组件注册为 Dapr 服务并使用 Dapr 客户端调用其 gRPC API。</p>
<p>关于 Dapr 运行的 AI 服务数量的配置，Dapr 提供了一种称为 “Actor” 的编程模型，它可以帮助开发人员轻松地构建高度可扩展的应用程序。Actor 是一种轻量级的计算单元，它可以自动缩放和分区，以处理大量的并发请求。可以使用 Dapr 的 Actor 模型来管理和配置 Dapr 运行的 AI 服务数量。</p>
<p>具体来说，可以使用 Dapr 的 Actor 模型来创建一个名为 “AI 服务” 的 Actor 组，该组可以包含多个 AI 服务 Actor 实例。然后，可以使用 Dapr 客户端调用 Actor 组的 API 来创建、删除和管理 AI 服务 Actor 实例。通过这种方式，可以手动配置 Dapr 运行的 AI 服务数量，并根据需要动态缩放。</p>
<h4 id="能否将tensorflow-serving-替换为torch-serving">能否将 tensorflow serving 替换为 torch serving</h4>
<p>关于将 TensorFlow Serving 替换为 Torch Serving，Dapr 并不限制使用哪种 AI 服务组件。只要 AI 服务组件实现了 Dapr 所需的 API，就可以将其注册为 Dapr 组件，并使用 Dapr 客户端调用其 API。因此，可以使用 Torch Serving 替换 TensorFlow Serving，并使用 Dapr 客户端调用其 API。</p>
<p>具体来说，需要将 Torch Serving 组件实现为 Dapr 组件，并将其注册到 Dapr 运行时中。然后，可以使用 Dapr 客户端调用 Torch Serving 组件的 API，就像在上述示例中使用 Dapr 客户端调用 TensorFlow Serving 组件的 API 一样。</p>
<h4 id="TensorFlow-Serving-和-Torch-Serving-的区别">TensorFlow Serving 和 Torch Serving 的区别</h4>
<p>TensorFlow Serving 和 Torch Serving 都是用于将机器学习模型部署为可用于生产环境的 API 的工具。它们的主要区别在于它们支持的框架不同。</p>
<p>TensorFlow Serving 是专门为 TensorFlow 模型设计的，它提供了一个高效的、可扩展的方式来部署 TensorFlow 模型。它支持多种模型格式，包括 SavedModel、SessionBundle 和 TensorFlow Hub 模块。TensorFlow Serving 还提供了一些高级功能，例如模型版本控制、模型热更新和模型分片。</p>
<p>Torch Serving 是专门为 PyTorch 模型设计的，它提供了一个轻量级的、可扩展的方式来部署 PyTorch 模型。它支持多种模型格式，包括 TorchScript 和 ONNX。Torch Serving 还提供了一些高级功能，例如模型版本控制、模型热更新和模型分片。</p>
<p>总的来说，TensorFlow Serving 和 Torch Serving 都是非常有用的工具，可以帮助将机器学习模型部署到生产环境中。选择哪个工具取决于使用的框架以及具体需求。</p>
<h3 id="AI-serving-app">AI serving app</h3>
<h4 id="flask-app">flask app</h4>
<p>实现功能：将 AI 服务的图片预测和输出显示到 web 页面</p>
<blockquote>
<p>[!NOTE]</p>
<ul class="lvl-1">
<li class="lvl-2">图片保存在本地子文件夹 <code>.predictions</code> 中以请求<code>检测时间</code>保存在对应子目录下。</li>
<li class="lvl-2">提供一个接口返回本地子文件夹<code>.predictions</code> 中的目录列表。</li>
<li class="lvl-2">返回页面增加以下 html 样式，方面人查询 <code>predictions</code> 中的图片结果。</li>
</ul>
</blockquote>
<p>为了将 AI 服务的图片预测和输出显示到 web 页面，需要使用一些 web 开发技术。以下是一些可能有用的步骤：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>创建一个 web 应用程序，该应用程序将接收用户上传的图像并将其发送到 AI 服务进行预测。可以使用任何 web 框架来实现这一点，例如 Flask 或 Django。</p>
</li>
<li class="lvl-2">
<p>在 web 应用程序中，使用 AI 服务客户端库将图像发送到 AI 服务进行预测。可以使用 Triton Serving 客户端库来实现这一点。有关如何使用 Triton Serving 客户端库的更多信息，请参见以下链接：<a target="_blank" rel="noopener" href="https://github.com/triton-inference-server/client">https://github.com/triton-inference-server/client</a></p>
</li>
<li class="lvl-2">
<p>将 AI 服务的预测结果返回到 web 应用程序，并将其显示在 web 页面上。可以使用 HTML 和 JavaScript 来实现这一点。例如，可以使用 JavaScript 将预测结果显示在一个图像元素中。</p>
</li>
</ul>
<p>以下是一个可能有用的代码示例，它演示了如何使用 Flask 和 Triton Serving 客户端库将图像发送到 AI 服务进行预测，并将预测结果显示在 web 页面上：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> io</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> pathlib</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> yolov5_utils  <span class="comment"># pip install yolov5-utils</span></span><br><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> Flask, request, render_template, send_file  <span class="comment"># pip install flask</span></span><br><span class="line"></span><br><span class="line">app = Flask(__name__)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ROOT = pathlib.Path((os.path.dirname(__file__)))</span><br><span class="line">PREDICTIONS = ROOT / <span class="string">".predictions"</span>  <span class="comment"># 预测保存文件夹</span></span><br><span class="line">DATE_FORMAT = <span class="string">"%Y-%m-%d_%H-%M-%S"</span>  <span class="comment"># 保存结果在子文件夹日期名称</span></span><br><span class="line">LIST_RENDER_FILE = <span class="string">"index.html"</span>  <span class="comment"># web 返回渲染页面</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_det_history_list</span>():</span><br><span class="line">    <span class="string">"""获取 PREDICTIONS 下的所有预测结果,并按日期最新,从左到右返回"""</span></span><br><span class="line">    <span class="keyword">return</span> [</span><br><span class="line">        os.path.relpath(p, PREDICTIONS)</span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> <span class="built_in">sorted</span>(PREDICTIONS.glob(<span class="string">"**/*"</span>), reverse=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">if</span> p.suffix <span class="keyword">in</span> {<span class="string">".jpg"</span>, <span class="string">".png"</span>}</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">"/infer"</span>, methods=[<span class="string">"POST"</span>]</span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">infer</span>():</span><br><span class="line">    <span class="string">"""推理接口, 只接受POST 请求,并将结果保存在 PREDICTIONS 下,并返回推理结果"""</span></span><br><span class="line">    data = request.files[<span class="string">"image"</span>].read()</span><br><span class="line">    sub = datetime.datetime.now().strftime(DATE_FORMAT)</span><br><span class="line">    results = model(Image.<span class="built_in">open</span>(io.BytesIO(data)), size=PREDICT_IMAGE_SIZE)</span><br><span class="line">    results.save(save_dir=PREDICTIONS / sub)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> render_template(</span><br><span class="line">        LIST_RENDER_FILE,</span><br><span class="line">        prediction=results,</span><br><span class="line">        image_list=get_det_history_list(),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">"/"</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">index</span>():</span><br><span class="line">    <span class="string">"""默认的 web 页面,返回检测历史结果"""</span></span><br><span class="line">    <span class="keyword">return</span> render_template(</span><br><span class="line">        LIST_RENDER_FILE,</span><br><span class="line">        image_list=get_det_history_list(),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">"/display/&lt;path:filename&gt;"</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">display</span>(<span class="params">filename</span>):</span><br><span class="line">    <span class="string">"""定于回调展示检测结果的图片</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        filename: 是 PREDICTIONS 下的图片的路径.(包括图片名)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> send_file(os.path.join(PREDICTIONS, filename), mimetype=<span class="string">"image/jpeg"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">parse_args</span>():</span><br><span class="line">    <span class="keyword">import</span> argparse</span><br><span class="line"></span><br><span class="line">    parser = argparse.ArgumentParser()</span><br><span class="line">    parser.add_argument(</span><br><span class="line">        <span class="string">"--model_url"</span>,</span><br><span class="line">        <span class="built_in">type</span>=<span class="built_in">str</span>,</span><br><span class="line">        default=<span class="string">"yolov5m.pt"</span>,</span><br><span class="line">        <span class="built_in">help</span>=<span class="string">"Triton model url such as http://localhost:8000/v2/models/yolov5,or yolov5 family name likes yolov5m.pt"</span>,</span><br><span class="line">    )</span><br><span class="line">    parser.add_argument(</span><br><span class="line">        <span class="string">"--imgsz"</span>,</span><br><span class="line">        <span class="built_in">type</span>=<span class="built_in">int</span>,</span><br><span class="line">        default=<span class="number">1280</span>,</span><br><span class="line">        <span class="built_in">help</span>=<span class="string">"The size of the image is detected after preprocessing"</span>,</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> parser.parse_args()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    FLAGS = parse_args()</span><br><span class="line">    PREDICT_IMAGE_SIZE = FLAGS.imgsz</span><br><span class="line">    model = yolov5_utils.load(FLAGS.model_url)  <span class="comment"># 加载 yolov5 模型</span></span><br><span class="line">    app.run(host=<span class="string">"0.0.0.0"</span>, port=<span class="number">5000</span>, debug=<span class="literal">False</span> <span class="keyword">if</span> sys.gettrace() <span class="keyword">else</span> <span class="literal">True</span>)</span><br></pre></td></tr></tbody></table></figure>
<p>这个示例使用 Flask 框架创建了一个 web 应用程序，当用户上传图像时，它将图像发送到 Triton Serving 进行预测，并将预测结果显示在 web 页面上。</p>
<p>创建了一个新的 HTML 模板 index.html 在 templates 目录下，它显示了 .predictions 子文件夹中的目录列表。</p>
<figure class="highlight html"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- index.html --&gt;</span></span><br><span class="line"><span class="meta">&lt;!DOCTYPE <span class="keyword">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">title</span>&gt;</span>Image Detection<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">style</span>&gt;</span><span class="language-css"></span></span><br><span class="line"><span class="language-css">        <span class="selector-class">.image</span> {</span></span><br><span class="line"><span class="language-css">            <span class="attribute">max-width</span>: <span class="number">100%</span>;</span></span><br><span class="line"><span class="language-css">            <span class="attribute">max-height</span>: <span class="number">100%</span>;</span></span><br><span class="line"><span class="language-css">            <span class="attribute">display</span>: block;</span></span><br><span class="line"><span class="language-css">            <span class="attribute">margin</span>: auto;</span></span><br><span class="line"><span class="language-css">        }</span></span><br><span class="line"><span class="language-css"></span></span><br><span class="line"><span class="language-css">        <span class="selector-class">.image-list</span> {</span></span><br><span class="line"><span class="language-css">            <span class="attribute">display</span>: flex;</span></span><br><span class="line"><span class="language-css">            <span class="attribute">flex-wrap</span>: wrap;</span></span><br><span class="line"><span class="language-css">        }</span></span><br><span class="line"><span class="language-css">    </span><span class="tag">&lt;/<span class="name">style</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">h1</span>&gt;</span>Image Detection Input<span class="tag">&lt;/<span class="name">h1</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 提交时将数据使用POST方法路由到 /infer --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">form</span> <span class="attr">method</span>=<span class="string">"POST"</span> <span class="attr">enctype</span>=<span class="string">"multipart/form-data"</span> <span class="attr">action</span>=<span class="string">"/infer"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"file"</span> <span class="attr">name</span>=<span class="string">"image"</span> <span class="attr">accept</span>=<span class="string">"image/*"</span> <span class="attr">required</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">button</span> <span class="attr">type</span>=<span class="string">"submit"</span>&gt;</span>Submit<span class="tag">&lt;/<span class="name">button</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">form</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 展示当前检测结果, 使用render_template传入的 prediction 变量展示 --&gt;</span></span><br><span class="line">    {% if prediction is defined %}</span><br><span class="line">    <span class="tag">&lt;<span class="name">h2</span>&gt;</span>Prediction: {{ prediction }}<span class="tag">&lt;/<span class="name">h2</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"image-list"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">img</span> <span class="attr">class</span>=<span class="string">"image"</span> <span class="attr">src</span>=<span class="string">"{{ url_for('display', filename=image_list[0] ) }}"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">    {% endif %}</span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 展示历史检测结果 --&gt;</span></span><br><span class="line">    {% if image_list is defined and image_list %}</span><br><span class="line">    <span class="tag">&lt;<span class="name">h2</span>&gt;</span>Detection History:<span class="tag">&lt;/<span class="name">h2</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span>&gt;</span></span><br><span class="line">        {% for file in image_list %}</span><br><span class="line">        <span class="tag">&lt;<span class="name">div</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">li</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"/display/{{ file }}"</span>&gt;</span>{{ file }}<span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">        {% endfor %}</span><br><span class="line">    <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">    {% endif %}</span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></tbody></table></figure>
<h2 id="Kubeflow">Kubeflow</h2>
<h3 id="install">install</h3>
<ul class="lvl-0">
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://github.com/myoh0623/kubeflow.git">myoh0623 install</a></p>
</li>
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://github.com/alauda/kubeflow-chart.git">alauda kubeflow chart</a></p>
</li>
</ul>
<h2 id="KServe">KServe</h2>
<h3 id="Quickstart">Quickstart</h3>
<ul class="lvl-0">
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://kserve.github.io/website/0.10/get_started/#install-the-kserve-quickstart-environment">experimental quickstart</a></p>
</li>
</ul>
<h3 id="modelmesh-installing">modelmesh installing</h3>
<ul class="lvl-0">
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://github.com/kserve/modelmesh-serving/blob/main/docs/quickstart.md">modelmesh quickstart install</a></p>
</li>
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://github.com/kserve/modelmesh-serving/blob/main/docs/install/install-script.md">standard install</a></p>
</li>
</ul>
<h3 id="Demo-Test">Demo Test</h3>
<ul class="lvl-0">
<li class="lvl-2">
<p>kerve quick start</p>
</li>
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://kserve.github.io/website/0.10/get_started/first_isvc/#run-your-first-inferenceservice">run-your-first-inferenceservice</a></p>
</li>
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://github.com/tduffy000/kfserving-uri-examples">kfserving-uri-examples</a></p>
</li>
</ul>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Minikube</span></span><br><span class="line"><span class="built_in">export</span> INGRESS_HOST=$(minikube ip)</span><br><span class="line"><span class="comment"># Other environment(On Prem)</span></span><br><span class="line"><span class="built_in">export</span> INGRESS_HOST=$(kubectl get po -l istio=ingressgateway -n istio-system -o jsonpath=<span class="string">'{.items[0].status.hostIP}'</span>)</span><br><span class="line"><span class="built_in">export</span> INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath=<span class="string">'{.spec.ports[?(@.name=="http2")].nodePort}'</span>)</span><br><span class="line"><span class="built_in">export</span> SERVICE_HOSTNAME=$(kubectl get inferenceservice sklearn-iris -n kserve-test -o jsonpath=<span class="string">'{.status.url}'</span> | <span class="built_in">cut</span> -d <span class="string">"/"</span> -f 3)</span><br><span class="line">curl -v -H <span class="string">"Host: <span class="variable">${SERVICE_HOSTNAME}</span>"</span> <span class="string">"http://<span class="variable">${INGRESS_HOST}</span>:<span class="variable">${INGRESS_PORT}</span>/v1/models/sklearn-iris:predict"</span> -d @./iris-input.json</span><br></pre></td></tr></tbody></table></figure>
<ul class="lvl-0">
<li class="lvl-2">
<p>kiali<br>
安装 istioctl</p>
</li>
</ul>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.15/samples/addons/kiali.yaml</span><br><span class="line">kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.15/samples/addons/prometheus.yaml</span><br></pre></td></tr></tbody></table></figure>
<h2 id="TAO">TAO</h2>
<p><a target="_blank" rel="noopener" href="https://developer.nvidia.com/tao-toolkit">NVIDIA TAO Toolkit</a> 是 Nvidia 构建的<a target="_blank" rel="noopener" href="https://blogs.nvidia.com/blog/2019/02/07/what-is-transfer-learning/">迁移学习</a>框架，能将现有的或合成数据使用预训练的模型高效地训练出自定义的模型，<a target="_blank" rel="noopener" href="https://docs.nvidia.com/tao/tao-toolkit/text/overview.html">官方介绍</a>。</p>
<ul class="lvl-0">
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/tao/tao-toolkit/index.html">TAO Toolkit Index</a></p>
</li>
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/tao/tao-toolkit/text/tao_toolkit_quick_start_guide.html">TAO Toolkit Quick Start Guide</a></p>
</li>
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/tao/tao-toolkit/text/tao_toolkit_api/api_overview.html">TAO Toolkit API</a></p>
</li>
</ul>
<h3 id="TAO-Toolkit-API">TAO Toolkit API</h3>
<ul class="lvl-0">
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/helm-charts/tao-toolkit-api">Helm Chart</a></p>
</li>
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/tao/tao-toolkit/text/tao_toolkit_api/api_deployment.html">API Deployment</a></p>
</li>
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/tao/tao-toolkit/text/automl/automl.html">AutoML</a></p>
</li>
</ul>
<h3 id="CV">CV</h3>
<ul class="lvl-0">
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/tao/tao-toolkit/text/offline_data_augmentation.html">Offline Data Augmentation</a></p>
</li>
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/tao/tao-toolkit/text/tensorboard_visualization.html">Tensorboard Visualization</a></p>
</li>
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/tao/tao-toolkit/text/data_annotation_format.html">Data Annotation Format</a></p>
</li>
</ul>
<h3 id="ClearML">ClearML</h3>
<ul class="lvl-0">
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/tao/tao-toolkit/text/mlops/clearml.html#tao-toolkit-clearml">ClearML Integration</a></p>
</li>
</ul>
<h3 id="Deploy">Deploy</h3>
<ul class="lvl-0">
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/tao/tao-toolkit/text/tao_cv_triton_inf_server/index.html">Triton Deploy</a></p>
<ul class="lvl-2">
<li class="lvl-4"><a target="_blank" rel="noopener" href="https://github.com/NVIDIA-AI-IOT/tao-toolkit-triton-apps">Triton Integration Examples</a></li>
</ul>
</li>
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/riva/user-guide/docs/overview.html">Riva</a></p>
</li>
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/tao/tao-toolkit/text/ds_tao/deepstream_tao_integration.html">DeepStream Integration</a></p>
</li>
</ul>
<h2 id="ClearML-2">ClearML</h2>
<p>ClearML 是一个开源的平台，用于管理和跟踪机器学习实验。它提供了一个集成的界面，可以帮助用户管理数据集、模型、超参数和实验结果。此外，ClearML 还提供了自动化的模型选择和调优功能，可以帮助用户快速构建和部署机器学习模型。</p>
<p>如果想了解更多关于 ClearML 的信息，可以参考以下文档：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://docs.clear.ml/">ClearML 官方文档</a></p>
</li>
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://github.com/allegroai/clearml">ClearML GitHub 仓库</a></p>
</li>
</ul>
<h2 id="pytorch">pytorch</h2>
<h3 id="Model-construct-strategy">Model construct strategy</h3>
<p>在 PyTorch 中构建模型的一般步骤如下：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>定义模型类：定义一个继承自 nn.Module 的类，该类将包含模型的结构和参数。</p>
</li>
<li class="lvl-2">
<p>定义前向传递函数：在模型类中定义一个 forward 函数，该函数将定义模型的前向传递逻辑。</p>
</li>
<li class="lvl-2">
<p>定义损失函数：选择适当的损失函数，例如交叉熵损失函数或均方误差损失函数。</p>
</li>
<li class="lvl-2">
<p>定义优化器：选择适当的优化器，例如随机梯度下降（SGD）或 Adam 优化器。</p>
</li>
<li class="lvl-2">
<p>训练模型：使用训练数据对模型进行训练，通过反向传播算法更新模型参数。</p>
</li>
<li class="lvl-2">
<p>评估模型：使用测试数据对模型进行评估，计算模型的准确率或其他性能指标。</p>
</li>
</ul>
<p>下面是一个简单的示例，展示如何在 PyTorch 中构建一个简单的全连接神经网络模型：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size, hidden_size, num_classes</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleNet, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.fc1 = nn.Linear(input_size, hidden_size)  <span class="comment"># 输入层</span></span><br><span class="line">        <span class="variable language_">self</span>.relu = nn.ReLU()  <span class="comment"># 隐藏层</span></span><br><span class="line">        <span class="variable language_">self</span>.fc2 = nn.Linear(hidden_size, num_classes)  <span class="comment"># 输出层</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        out = <span class="variable language_">self</span>.fc1(x)</span><br><span class="line">        out = <span class="variable language_">self</span>.relu(out)</span><br><span class="line">        out = <span class="variable language_">self</span>.fc2(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义模型参数</span></span><br><span class="line">input_size = <span class="number">784</span>  <span class="comment"># 输入大小为28x28</span></span><br><span class="line">hidden_size = <span class="number">500</span></span><br><span class="line">num_classes = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化模型</span></span><br><span class="line">model = SimpleNet(input_size, hidden_size, num_classes)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数和优化器</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="comment"># 前向传递</span></span><br><span class="line">    outputs = model(inputs)</span><br><span class="line">    loss = criterion(outputs, labels)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 反向传播和优化</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 评估模型</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    total = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> images, labels <span class="keyword">in</span> test_loader:</span><br><span class="line">        images = images.reshape(-<span class="number">1</span>, <span class="number">28</span> * <span class="number">28</span>)</span><br><span class="line">        outputs = model(images)</span><br><span class="line">        _, predicted = torch.<span class="built_in">max</span>(outputs.data, <span class="number">1</span>)</span><br><span class="line">        total += labels.size(<span class="number">0</span>)</span><br><span class="line">        correct += (predicted == labels).<span class="built_in">sum</span>().item()</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(</span><br><span class="line">        <span class="string">"Accuracy of the network on the 10000 test images: %d %%"</span></span><br><span class="line">        % (<span class="number">100</span> * correct / total)</span><br><span class="line">    )</span><br></pre></td></tr></tbody></table></figure>
<p>在这个示例中，定义了一个名为 SimpleNet 的模型类，它包含一个输入层、一个隐藏层和一个输出层。还定义了一个前向传递函数，该函数将输入数据传递到模型中，并返回模型的输出。使用交叉熵损失函数和随机梯度下降（SGD）优化器来训练模型，并使用测试数据对模型进行评估。</p>
<h3 id="Common-API">Common API</h3>
<h4 id="tensor-equal">tensor.equal</h4>
<p>使用 torch.equal () 函数来判断两个 tensor 是否相等。该函数会比较两个 tensor 的形状和元素值是否相等，如果相等则返回 True，否则返回 False。</p>
<p>示例代码如下：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">a = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">b = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">c = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">4</span>])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.equal(a, b))  <span class="comment"># True</span></span><br><span class="line"><span class="built_in">print</span>(torch.equal(a, c))  <span class="comment"># False</span></span><br></pre></td></tr></tbody></table></figure>
<p>在上面的示例中，定义了三个 tensor，其中 a 和 b 的元素值和形状都相等，而 c 的元素值不同，因此使用 torch.equal () 函数比较 a 和 b 时返回 True，比较 a 和 c 时返回 False。</p>
<h4 id="tensor-view">tensor.view</h4>
<p>tensor.view 是 PyTorch 中的一个方法，用于将一个 tensor 重塑为指定的形状。它返回一个新的 tensor，而不是修改原始 tensor。这个方法在深度学习中非常常用，因为在神经网络中，需要将数据重塑为特定的形状以进行计算。</p>
<p>tensor.view 的语法如下：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">view(*shape)</span><br></pre></td></tr></tbody></table></figure>
<p>其中，*shape 是一个可变参数，表示要重塑的形状。例如，如果有一个形状为 (3, 4) 的 tensor，可以使用 view 将其重塑为 (4, 3) 的 tensor：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x = torch.randn(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">y = x.view(<span class="number">4</span>, <span class="number">3</span>)</span><br></pre></td></tr></tbody></table></figure>
<p>需要注意的是，view 方法返回的 tensor 与原始 tensor 共享数据存储空间，因此在修改其中一个 tensor 的值时，另一个 tensor 的值也会发生变化。如果要避免这种情况，可以使用 clone 方法创建一个新的 tensor。</p>
<p>除了 view 方法，PyTorch 还提供了一些其他的 tensor 重塑方法，例如 reshape 和 permute。这些方法的使用方式与 view 类似，但有一些细微的差别，需要根据具体情况选择合适的方法。</p>
<h4 id="tensor-reshape">tensor.reshape</h4>
<p>pytorch tensor.reshape 是 PyTorch 中用于改变张量形状的函数之一。它可以将一个张量重塑为一个新的形状，而不改变其元素的数量或值。在 PyTorch 中，张量的形状可以通过 size () 函数获取，也可以通过 reshape () 函数修改。</p>
<p>reshape () 函数的语法如下：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">new_tensor = tensor.reshape(*shape)</span><br></pre></td></tr></tbody></table></figure>
<p>其中，tensor 是要重塑的张量，shape 是一个整数元组，表示新张量的形状。新张量的元素数量必须与原张量的元素数量相同，否则会引发错误。</p>
<p>例如，将一个形状为 (3, 4) 的张量重塑为一个形状为 (4, 3) 的张量，可以使用以下代码：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个形状为 (3, 4) 的张量</span></span><br><span class="line">tensor = torch.randn(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将张量重塑为形状为 (4, 3) 的新张量</span></span><br><span class="line">new_tensor = tensor.reshape(<span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印新张量的形状</span></span><br><span class="line"><span class="built_in">print</span>(new_tensor.size())  <span class="comment"># 输出: torch.Size([4, 3])</span></span><br></pre></td></tr></tbody></table></figure>
<p>需要注意的是，reshape () 函数返回的是一个新的张量，而不是修改原始张量。如果需要修改原始张量的形状，可以使用 view () 函数。view () 函数与 reshape () 函数的功能相同，但是它只能用于连续的内存块，而 reshape () 函数可以用于任意张量。</p>
<h4 id="tensor-permute">tensor.permute</h4>
<p>tensor.permute () 是 PyTorch 中的一个函数，用于对张量进行维度重排。它可以用于将张量从一种格式转换为另一种格式，例如从 NHWC 格式转换为 NCHW 格式。</p>
<p>tensor.permute () 的参数是一个整数列表，用于指定新张量的维度顺序。例如，如果原始张量的维度顺序为 (batch_size, height, width, channels)，而新张量的维度顺序为 (batch_size, channels, height, width)，则可以使用以下代码进行转换：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert from NHWC to NCHW with torch.Tensor.permute</span></span><br><span class="line">tensor_nchw = tensor_nhwc.permute(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br></pre></td></tr></tbody></table></figure>
<p>在这里，tensor_nhwc 是一个形状为 (batch_size, height, width, channels) 的张量。tensor_nchw 是一个形状为 (batch_size, channels, height, width) 的张量，其中通道维度在第二个维度。tensor_nchw.permute (0, 3, 1, 2) 将张量的维度重新排列，以将通道维度移动到第二个维度。</p>
<p>需要注意的是，tensor.permute () 返回的是一个新的张量，而不是修改原始张量。因此，如果需要在原始张量上进行修改，可以使用 tensor.transpose () 函数。tensor.transpose () 的参数是一个整数列表，用于指定新张量的维度顺序。例如，如果需要将张量的前两个维度交换，则可以使用以下代码：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># Swap the first two dimensions with torch.Tensor.transpose</span></span><br><span class="line">tensor_swapped = tensor.transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br></pre></td></tr></tbody></table></figure>
<p>在这里，tensor 是一个形状为 (dim1, dim2, …) 的张量。tensor_swapped 是一个形状为 (dim2, dim1, …) 的张量，其中原始张量的前两个维度已经交换。</p>
<p>总之，tensor.permute () 和 tensor.transpose () 都是用于对张量进行维度重排的函数，可以根据需要选择使用。</p>
<h4 id="tensor-transpose">tensor.transpose</h4>
<p>transpose 是 PyTorch 中的一个函数，用于对张量进行转置操作。它可以接受一个元组作为参数，表示需要交换的维度。例如，对于一个形状为 (3, 4, 5) 的张量，可以使用 transpose (0, 2, 1) 将第一维和第三维交换，得到一个形状为 (5, 4, 3) 的张量。</p>
<p>下面是一个示例代码，展示了如何使用 transpose 函数：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个形状为 (3, 4, 5) 的张量</span></span><br><span class="line">x = torch.randn(<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对张量进行转置操作</span></span><br><span class="line">y = x.transpose(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印转置后的张量形状</span></span><br><span class="line"><span class="built_in">print</span>(y.shape)</span><br></pre></td></tr></tbody></table></figure>
<p>输出结果为 (5, 4, 3)，表示转置后的张量形状为 (5, 4, 3)。</p>
<p>需要注意的是，transpose 函数返回的是一个新的张量，不会修改原始张量。如果需要在原始张量上进行转置操作，可以使用 transpose_ 函数。</p>
<h4 id="tensor-topk">tensor.topk</h4>
<p>使用 tensor.topk 函数可以返回张量中前 k 个最大值及其对应的索引。该函数的语法如下：</p>
<figure class="highlight txt"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.topk(input, k, dim=None, largest=True, sorted=True, out=None) -&gt; Tuple[Tensor, LongTensor]</span><br></pre></td></tr></tbody></table></figure>
<p>其中，参数含义如下：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>input：输入张量。</p>
</li>
<li class="lvl-2">
<p>k：需要返回的最大值的个数。</p>
</li>
<li class="lvl-2">
<p>dim：沿着哪个维度计算，默认为最后一个维度。</p>
</li>
<li class="lvl-2">
<p>largest：如果为 True，则返回最大的 k 个值；如果为 False，则返回最小的 k 个值。</p>
</li>
<li class="lvl-2">
<p>sorted：如果为 True，则返回的 k 个值是有序的；如果为 False，则返回的 k 个值是无序的。</p>
</li>
<li class="lvl-2">
<p>out：输出张量。</p>
</li>
</ul>
<p>下面是一个简单的示例：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line">values, indices = torch.topk(x, k=<span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(values)  <span class="comment"># tensor([5, 4, 3])</span></span><br><span class="line"><span class="built_in">print</span>(indices)  <span class="comment"># tensor([4, 3, 1])</span></span><br></pre></td></tr></tbody></table></figure>
<p>在这个示例中，创建了一个张量 x，然后使用 torch.topk 函数返回了 x 中前 3 个最大值及其对应的索引。输出结果中，values 是一个包含前 3 个最大值的张量，indices 是一个包含前 3 个最大值对应的索引的张量。</p>
<h4 id="tensor-softmax">tensor.softmax</h4>
<p>tensor.softmax 是 PyTorch 中的一个函数，用于计算张量的 softmax 函数。softmax 函数是一种常用的激活函数，通常用于多分类问题中，将输出的原始分数转换为概率分布。softmax 函数的公式如下：</p>
<p>$$<br>
\text{softmax}(x_i) = \frac{e<sup>{x_i}}{\sum_{j=1}</sup>{n} e^{x_j}}<br>
$$</p>
<p>其中，$x_i$ 表示输入张量的第 $i$ 个元素，$n$ 表示张量的长度。</p>
<p>在 PyTorch 中，可以使用 torch.tensor.softmax 函数计算张量的 softmax 函数。该函数的语法如下：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor.softmax(<span class="built_in">input</span>, dim=<span class="literal">None</span>, dtype=<span class="literal">None</span>)</span><br></pre></td></tr></tbody></table></figure>
<p>其中，input 表示输入的张量，dim 表示计算 softmax 函数的维度，dtype 表示输出的数据类型。如果 dim 参数未指定，则默认计算最后一维的 softmax 函数。</p>
<p>下面是一个示例代码：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x = torch.tensor([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>])</span><br><span class="line">y = torch.tensor([[-<span class="number">1.0</span>, <span class="number">0.0</span>, <span class="number">1.0</span>], [<span class="number">2.0</span>, <span class="number">3.0</span>, <span class="number">4.0</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算一维张量的 softmax 函数</span></span><br><span class="line">x_softmax = torch.tensor.softmax(x)</span><br><span class="line"><span class="built_in">print</span>(x_softmax)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算二维张量的 softmax 函数</span></span><br><span class="line">y_softmax = torch.tensor.softmax(y, dim=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(y_softmax)</span><br><span class="line"></span><br><span class="line"><span class="comment"># tensor([0.0900, 0.2447, 0.6652])</span></span><br><span class="line"><span class="comment"># tensor([[0.0900, 0.2447, 0.6652],</span></span><br><span class="line"><span class="comment">#         [0.0900, 0.2447, 0.6652]])</span></span><br></pre></td></tr></tbody></table></figure>
<p>在上面的示例代码中，分别计算了一个一维张量和一个二维张量的 softmax 函数，并输出了计算结果。</p>
<h4 id="torch-no-gard">torch.no_gard</h4>
<p>在深度学习中，torch.no_grad () 是一个上下文管理器，用于控制是否计算梯度。当不需要计算梯度时，可以使用它来提高代码的执行效率和节省内存。</p>
<p>在训练模型时，通常会使用反向传播算法计算梯度，并使用梯度来更新模型的参数。然而，在推理阶段或者评估模型性能时，通常不需要计算梯度，只需要使用模型进行前向传播，以获得预测结果。</p>
<p>torch.no_grad () 的作用就是告诉 PyTorch 不要计算梯度，从而避免不必要的计算和内存消耗。通过在代码中使用 torch.no_grad ()，可以明确地指示 PyTorch 不要跟踪梯度信息，从而提高代码的执行效率。</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> torch.no_gard():</span><br><span class="line">    ...</span><br></pre></td></tr></tbody></table></figure>
<blockquote>
<p>[!NOTE]<br>
<code>torch&gt;=1.9.0</code> 中已将 torch.no_gard 替换为 torch.inference_mode</p>
</blockquote>
<h4 id="torch-matmul">torch.matmul</h4>
<p>以下给出一个公式的示例:</p>
<figure class="highlight txt"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">output = i*weight0* weight^T</span><br></pre></td></tr></tbody></table></figure>
<p>output = i<em>weight0</em> weight^T 表示的是一个线性变换的过程，其中 i 是输入张量，weight0 和 weight 是权重矩阵。在这个公式中，weight0 和 weight 的乘积可以看作是一个新的权重矩阵，可以将其记为 new_weight，即 new_weight = weight0 <em>weight^T。那么原始的公式可以改写为 output = i</em> new_weight。</p>
<p>如果想要优先计算 new_weight，可以先将 weight0 和 weight 相乘得到 new_weight，然后再将 i 与 new_weight 相乘得到输出张量 output。这样做是合理的，因为矩阵乘法具有结合律，即 (A <em>B)</em> C = A <em>(B</em> C)，所以先计算 weight0 <em>weight^T 得到 new_weight，再计算 i</em> new_weight 得到 output，与直接计算 i <em>weight0</em> weight^T 得到 output 是等价的。</p>
<p>以下是一个示例代码，演示如何先计算 new_weight，然后再计算 output：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义输入张量 i，形状为 [batch_size, input_size]</span></span><br><span class="line">i = torch.randn(<span class="number">32</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义权重矩阵 weight，形状为 [output_size, input_size]</span></span><br><span class="line">weight = torch.randn(<span class="number">5</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义权重矩阵 weight0，形状为 [output_size, output_size]</span></span><br><span class="line">weight0 = torch.randn(<span class="number">5</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算 new_weight，形状为 [output_size, input_size]</span></span><br><span class="line">new_weight = torch.matmul(weight0, weight.transpose(<span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算 output，形状为 [batch_size, output_size]</span></span><br><span class="line">output = torch.matmul(i, new_weight)</span><br></pre></td></tr></tbody></table></figure>
<p>在这个例子中，先计算了 new_weight，然后再计算了 output。注意，在计算 new_weight 时，需要将 weight 的维度转置，以便与 weight0 相乘。最后，使用 PyTorch 中的 torch.matmul 函数进行矩阵乘法运算。</p>
<h4 id="nn-Linear">nn.Linear</h4>
<p>nn.Linear 是 PyTorch 中的一个线性层，它实现了一个线性变换，将输入张量与权重矩阵相乘并加上偏置向量。它的定义如下：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Linear</span>(<span class="title class_ inherited__">Module</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        in_features: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">        out_features: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">        bias: <span class="built_in">bool</span> = <span class="literal">True</span>,</span></span><br><span class="line"><span class="params">        device=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        dtype=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    </span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        ...</span><br></pre></td></tr></tbody></table></figure>
<p>在深度学习中，线性变换层是非常常见的一种操作，它可以将输入张量映射到一个新的空间中，从而实现特征的提取和转换。nn.Linear 的实现原理非常简单，它实际上就是一个矩阵乘法加上一个偏置向量的过程，可以用以下公式表示：</p>
<figure class="highlight txt"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">output = input * weight^T + bias</span><br></pre></td></tr></tbody></table></figure>
<p>其中，input 是输入张量，weight 是权重矩阵，bias 是偏置向量，^T 表示矩阵的转置操作。</p>
<p>其中，in_features 表示输入张量的大小，out_features 表示输出张量的大小，bias 表示是否使用偏置向量。在实际使用中，可以通过调用 nn.Linear 的实例对象来进行线性变换，例如：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个线性层，输入大小为 10，输出大小为 5</span></span><br><span class="line">linear = nn.Linear(<span class="number">10</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个输入张量，大小为 (batch_size, 10)</span></span><br><span class="line">input_tensor = torch.randn(<span class="number">32</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行线性变换</span></span><br><span class="line">output_tensor = linear(input_tensor)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出张量的大小为 (batch_size, 5)</span></span><br><span class="line"><span class="built_in">print</span>(output_tensor.size())</span><br></pre></td></tr></tbody></table></figure>
<p>在上述代码中，首先定义了一个输入大小为 10，输出大小为 5 的线性层 linear，然后定义了一个输入张量 input_tensor，大小为 (batch_size, 10)，最后通过调用 linear 对输入张量进行线性变换，得到输出张量 output_tensor，大小为 (batch_size, 5)。</p>
<p>需要注意的是，nn.Linear 的权重矩阵和偏置向量是在初始化时随机生成的，可以通过 linear.weight 和 linear.bias 来访问它们。此外，还可以通过 nn.init 模块来初始化权重矩阵和偏置向量，例如：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.init <span class="keyword">as</span> init</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个线性层，输入大小为 10，输出大小为 5</span></span><br><span class="line">linear = nn.Linear(<span class="number">10</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化权重矩阵和偏置向量</span></span><br><span class="line">init.normal_(linear.weight, mean=<span class="number">0</span>, std=<span class="number">0.01</span>)</span><br><span class="line">init.constant_(linear.bias, val=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行线性变换</span></span><br><span class="line">output_tensor = linear(input_tensor)</span><br></pre></td></tr></tbody></table></figure>
<p>在上述代码中，使用 init.normal_和 init.constant_ 分别对权重矩阵和偏置向量进行了初始化。其中，init.normal_表示使用正态分布进行初始化，init.constant_ 表示使用常数进行初始化。</p>
<h4 id="nn-Dropout">nn.Dropout</h4>
<p>nn.Dropout 是 PyTorch 中的一个模块，用于在神经网络中进行 Dropout 操作。Dropout 是一种常用的正则化方法，可以有效地防止过拟合。</p>
<p>在神经网络中，Dropout 会随机地将一些神经元的输出置为 0，从而减少神经元之间的依赖关系，使得网络更加鲁棒。具体来说，Dropout 会在训练过程中以一定的概率 p 随机地将某些神经元的输出置为 0，而在测试过程中则不进行 Dropout 操作。</p>
<p>在 PyTorch 中，可以通过如下方式使用 nn.Dropout：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Net, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.fc1 = nn.Linear(<span class="number">10</span>, <span class="number">20</span>)</span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(p=<span class="number">0.5</span>)</span><br><span class="line">        <span class="variable language_">self</span>.fc2 = nn.Linear(<span class="number">20</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.fc1(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.dropout(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.fc2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></tbody></table></figure>
<p>在上述代码中，定义了一个包含两个全连接层的神经网络，并在第一个全连接层后添加了一个 Dropout 模块。在 nn.Dropout 的构造函数中，可以指定 Dropout 的概率 p。在 forward 函数中，首先将输入 x 传入第一个全连接层，然后将输出传入 Dropout 模块，最后再将 Dropout 模块的输出传入第二个全连接层。</p>
<p>需要注意的是，当使用 nn.Dropout 时，需要在训练过程中启用 Dropout，而在测试过程中禁用 Dropout。在 PyTorch 中，可以通过调用 model.train () 和 model.eval () 方法来切换模型的训练和测试模式。具体来说，当调用 model.train () 方法时，模型会进入训练模式，此时 Dropout 模块会生效；而当调用 model.eval () 方法时，模型会进入测试模式，此时 Dropout 模块会被禁用。</p>
<h4 id="nn-Embedding">nn.Embedding</h4>
<p>nn.Embedding 是 PyTorch 中的一个类，用于实现词嵌入（word embedding）。词嵌入是一种将文本中的单词映射到低维向量空间中的技术，它可以将单词之间的语义关系转化为向量空间中的几何关系，从而方便计算机对文本进行处理和分析。</p>
<p>在 PyTorch 中，可以使用 nn.Embedding 类来实现词嵌入。该类的输入是一个整数张量，表示文本中的单词序列，输出是一个浮点数张量，表示单词序列中每个单词对应的词向量。在创建 nn.Embedding 对象时，需要指定词汇表的大小和词向量的维度。例如，下面的代码创建了一个词汇表大小为 10000，词向量维度为 300 的词嵌入对象：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">embedding = nn.Embedding(<span class="number">10000</span>, <span class="number">300</span>)</span><br></pre></td></tr></tbody></table></figure>
<p>在使用 nn.Embedding 对象时，可以将整数张量作为输入，得到对应的词向量。例如，下面的代码将一个大小为 (3, 4) 的整数张量作为输入，得到一个大小为 (3, 4, 300) 的浮点数张量，表示输入张量中每个整数对应的词向量：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">input_tensor = torch.randint(<span class="number">0</span>, <span class="number">10000</span>, (<span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line">output_tensor = embedding(input_tensor)</span><br><span class="line"><span class="built_in">print</span>(output_tensor.shape)  <span class="comment"># torch.Size([3, 4, 300])</span></span><br></pre></td></tr></tbody></table></figure>
<p>在实际使用中，词嵌入通常作为深度学习模型的输入层，用于将文本数据转换为向量表示。例如，在文本分类任务中，可以使用词嵌入将文本数据转换为向量表示，然后将向量输入到深度学习模型中进行分类。</p>
<h4 id="nn-modules-loss-CrossEntropyLoss">nn.modules.loss.CrossEntropyLoss</h4>
<p>nn.modules.loss.CrossEntropyLoss 是一个用于多分类问题的损失函数，它将 softmax 函数和负对数似然损失结合在一起。在训练分类模型时，通常使用交叉熵损失函数来衡量模型的预测结果与真实标签之间的差异。</p>
<p>在 PyTorch 中，nn.modules.loss.CrossEntropyLoss 的使用非常简单。只需要将模型的输出和真实标签传递给该函数即可。例如</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define the loss function</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define the model</span></span><br><span class="line">model = ...</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define the optimizer</span></span><br><span class="line">optimizer = ...</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train the model</span></span><br><span class="line"><span class="keyword">for</span> inputs, labels <span class="keyword">in</span> dataloader:</span><br><span class="line">    <span class="comment"># Forward pass</span></span><br><span class="line">    outputs = model(inputs)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute the loss , and shape must be</span></span><br><span class="line">    <span class="comment"># same between outputs and labels</span></span><br><span class="line">    loss = criterion(outputs, labels)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backward pass</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></tbody></table></figure>
<p>在上面的代码中，outputs 是模型的输出，labels 是真实标签。将这两个参数传递给 nn.modules.loss.CrossEntropyLoss 函数，它将计算模型的预测结果和真实标签之间的交叉熵损失。然后，可以使用该损失来进行反向传播和优化。</p>
<p>需要注意的是，nn.modules.loss.CrossEntropyLoss 函数要求模型的输出是未经过 softmax 函数处理的原始值。因此，在模型的最后一层中，通常不会使用 softmax 函数。如果使用了 softmax 函数，那么需要在调用 nn.modules.loss.CrossEntropyLoss 函数之前将模型的输出转换为未经过 softmax 函数处理的原始值。</p>
<h2 id="tensorflow">tensorflow</h2>
<h3 id="Model-construct-strategy-2">Model construct strategy</h3>
<ul class="lvl-0">
<li class="lvl-2">
<p>准备数据集：将数据集加载到内存中，并将其划分为训练集、验证集和测试集。</p>
</li>
<li class="lvl-2">
<p>定义模型：使用 TensorFlow 2 中的 Keras API 定义模型架构，包括层和激活函数等。</p>
</li>
<li class="lvl-2">
<p>编译模型：指定损失函数、优化器和评估指标等。</p>
</li>
<li class="lvl-2">
<p>训练模型：使用训练集训练模型，并在验证集上进行验证。</p>
</li>
<li class="lvl-2">
<p>评估模型：使用测试集评估模型的性能。</p>
</li>
<li class="lvl-2">
<p>使用模型：使用模型进行预测或推理。</p>
</li>
</ul>
<p>下面是一个使用 TensorFlow 2 构建简单神经网络的示例代码：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers</span><br><span class="line"></span><br><span class="line"><span class="comment"># 准备数据集</span></span><br><span class="line">(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()</span><br><span class="line">x_train = x_train.reshape(-<span class="number">1</span>, <span class="number">28</span> * <span class="number">28</span>).astype(<span class="string">"float32"</span>) / <span class="number">255.0</span></span><br><span class="line">x_test = x_test.reshape(-<span class="number">1</span>, <span class="number">28</span> * <span class="number">28</span>).astype(<span class="string">"float32"</span>) / <span class="number">255.0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line">model = tf.keras.Sequential(</span><br><span class="line">    [</span><br><span class="line">        layers.Dense(<span class="number">256</span>, activation=<span class="string">"relu"</span>),</span><br><span class="line">        layers.Dense(<span class="number">128</span>, activation=<span class="string">"relu"</span>),</span><br><span class="line">        layers.Dense(<span class="number">10</span>, activation=<span class="string">"softmax"</span>),</span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 编译模型</span></span><br><span class="line">model.<span class="built_in">compile</span>(</span><br><span class="line">    optimizer=tf.keras.optimizers.Adam(<span class="number">0.001</span>),</span><br><span class="line">    loss=tf.keras.losses.SparseCategoricalCrossentropy(),</span><br><span class="line">    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">model.fit(x_train, y_train, batch_size=<span class="number">32</span>, epochs=<span class="number">5</span>, validation_data=(x_test, y_test))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 评估模型</span></span><br><span class="line">model.evaluate(x_test, y_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用模型进行预测</span></span><br><span class="line">predictions = model.predict(x_test[:<span class="number">10</span>])</span><br></pre></td></tr></tbody></table></figure>
<p>在这个示例中，使用了 TensorFlow 2 中的 Keras API 定义了一个包含 3 个全连接层的神经网络，用于对 MNIST 手写数字数据集进行分类。使用了 Adam 优化器、交叉熵损失函数和稀疏分类精度评估指标来编译模型。然后，使用训练集训练模型，并在验证集上进行验证。最后，使用测试集评估了模型的性能，并使用模型进行了预测。</p>
<h3 id="Common-api">Common api</h3>
<h4 id="tf-trainable-variables">tf.trainable_variables()</h4>
<p>tf.trainable_variables () 是 TensorFlow 中的一个函数，用于获取所有可训练的变量。具体来说，它会返回一个列表，其中包含了所有需要在训练过程中更新的变量。</p>
<p>该函数会返回一个列表，其中包含了所有需要在训练过程中更新的变量。这些变量通常是模型中的权重和偏置等参数。在训练过程中，需要根据损失函数的梯度来更新这些变量，以使得模型能够更好地拟合训练数据。</p>
<p>需要注意的是，tf.trainable_variables () 只会返回需要在训练过程中更新的变量。如果一个变量不需要在训练过程中更新，那么它就不会出现在这个列表中。例如，如果一个变量是在模型中用于计算某个中间结果的，那么它就不需要在训练过程中更新，因此也不会出现在这个列表中。</p>
<p>在实际使用中，通常会将这个列表作为优化器的参数，以便优化器能够自动更新这些变量。例如：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">optimizer = tf.train.AdamOptimizer(learning_rate=<span class="number">0.001</span>)</span><br><span class="line">trainable_vars = tf.trainable_variables()</span><br><span class="line">train_op = optimizer.minimize(loss, var_list=trainable_vars)</span><br></pre></td></tr></tbody></table></figure>
<p>在这个例子中，使用 Adam 优化器来最小化损失函数 loss，并且将所有可训练的变量作为优化器的参数传入。这样，优化器就会自动更新这些变量，以使得模型能够更好地拟合训练数据。</p>
<h3 id="Adam">Adam</h3>
<p>Adam 是一种优化算法，用于训练神经网络。它是一种基于梯度下降的算法，可以自适应地调整每个参数的学习率。Adam 算法结合了动量方法和 RMSProp 方法的优点，因此在训练深度神经网络时表现良好。</p>
<p>在 TensorFlow 中，可以使用 tf.keras.optimizers.Adam 来使用 Adam 优化器。例如，以下代码演示了如何使用 Adam 优化器来训练一个简单的神经网络：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define the model</span></span><br><span class="line">model = tf.keras.Sequential(</span><br><span class="line">    [</span><br><span class="line">        tf.keras.layers.Dense(<span class="number">64</span>, activation=<span class="string">"relu"</span>),</span><br><span class="line">        tf.keras.layers.Dense(<span class="number">10</span>),</span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define the optimizer</span></span><br><span class="line">optimizer = tf.keras.optimizers.Adam(learning_rate=<span class="number">0.001</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compile the model</span></span><br><span class="line">model.<span class="built_in">compile</span>(</span><br><span class="line">    optimizer=optimizer,</span><br><span class="line">    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=<span class="literal">True</span>),</span><br><span class="line">    metrics=[<span class="string">"accuracy"</span>],</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train the model</span></span><br><span class="line">model.fit(x_train, y_train, epochs=<span class="number">10</span>, validation_data=(x_test, y_test))</span><br></pre></td></tr></tbody></table></figure>
<p>在这里，首先定义了一个简单的神经网络模型，然后定义了一个 Adam 优化器，并将其传递给模型的 compile 方法。最后，使用 fit 方法来训练模型。</p>
<p>需要注意的是，Adam 优化器有许多可调参数，例如学习率、beta1 和 beta2 等。在实践中，通常需要对这些参数进行调整，以获得最佳的性能。</p>
<h3 id="Tips-2">Tips</h3>
<h4 id="adam-m-and-adam-v">adam_m and adam_v</h4>
<p>如果 adam_m 和 adam_v 是在训练过程中使用的，那么在 eval model 中可能不需要这些变量。因为在 eval model 中，只需要使用训练好的模型进行预测，而不需要再进行训练。</p>
<h4 id="display-with-tensorboard">display with tensorboard</h4>
<p>展示 tensorflow 1 checkpoint</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载图</span></span><br><span class="line">graph = tf.Graph()</span><br><span class="line"><span class="keyword">with</span> graph.as_default():</span><br><span class="line">    tf.compat.v1.train.import_meta_graph(<span class="string">"./model.ckpt.meta"</span>)</span><br><span class="line"><span class="comment"># 显示图中的所有操作</span></span><br><span class="line"></span><br><span class="line">writer = tf.summary.create_file_writer(<span class="string">"logs"</span>)</span><br><span class="line"><span class="keyword">with</span> writer.as_default():</span><br><span class="line">    tf.summary.graph(graph)</span><br><span class="line">writer.close()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动 tensorboard,并打开localhost:6001</span></span><br><span class="line"><span class="comment"># tensorboard --logdir=logs</span></span><br></pre></td></tr></tbody></table></figure>
<h2 id="HuggingFace">HuggingFace</h2>
<p>HuggingFace 是一个提供自然语言处理模型和工具的开源社区。它提供了基于 transformers 的许多预训练的模型，包括 BERT、GPT-2、RoBERTa 等，这些模型可以用于各种 NLP 任务，例如文本分类、命名实体识别、情感分析等。使用 HuggingFace，可以轻松地加载这些预训练的模型，并在自己的数据集上进行微调。</p>
<ul class="lvl-0">
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://huggingface.co/docs">huggingface docs</a></p>
</li>
</ul>
<h3 id="Transformers">Transformers</h3>
<hr>
<p><a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/index">Transformers</a> 是一个用于自然语言处理的 Python 库，它提供了许多预训练的模型，包括 BERT、GPT-2、RoBERTa 等。这些模型可以用于各种 NLP 任务，例如文本分类、命名实体识别、情感分析等。使用 Transformers 库，可以轻松地加载这些预训练的模型，并在自己的数据集上进行微调。</p>
<p>以下是一个使用 Transformers 库进行文本分类的示例：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer, TFBertForSequenceClassification</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load the tokenizer and model</span></span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(<span class="string">"bert-base-uncased"</span>)</span><br><span class="line">model = TFBertForSequenceClassification.from_pretrained(<span class="string">"bert-base-uncased"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compile the model</span></span><br><span class="line">optimizer = tf.keras.optimizers.Adam(learning_rate=<span class="number">2e-5</span>)</span><br><span class="line">loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=<span class="literal">True</span>)</span><br><span class="line">metric = tf.keras.metrics.SparseCategoricalAccuracy(<span class="string">"accuracy"</span>)</span><br><span class="line">model.<span class="built_in">compile</span>(optimizer=optimizer, loss=loss, metrics=[metric])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load the data</span></span><br><span class="line">train_dataset = ...</span><br><span class="line">val_dataset = ...</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train the model</span></span><br><span class="line">model.fit(train_dataset, epochs=<span class="number">3</span>, validation_data=val_dataset)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Evaluate the model</span></span><br><span class="line">test_dataset = ...</span><br><span class="line">model.evaluate(test_dataset)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Make predictions</span></span><br><span class="line">text = <span class="string">"This is a test sentence."</span></span><br><span class="line">inputs = tokenizer.encode_plus(text, return_tensors=<span class="string">"tf"</span>)</span><br><span class="line">outputs = model(inputs)[<span class="number">0</span>]</span><br><span class="line">predictions = tf.argmax(outputs, axis=<span class="number">1</span>).numpy()</span><br></pre></td></tr></tbody></table></figure>
<p>在这个例子中，首先加载了 BERT 模型和对应的 tokenizer。然后，使用 TFBertForSequenceClassification 类来定义一个文本分类模型，并使用 Adam 优化器、交叉熵损失函数和稀疏分类精度评估指标来编译模型。接下来，加载了训练、验证和测试数据集，并使用 fit 方法来训练模型。最后，使用 evaluate 方法来评估模型的性能，并使用模型进行了预测。</p>
<p>需要注意的是，Transformers 库提供了许多不同的预训练模型和 tokenizer，可以根据具体任务的需求进行选择。此外，Transformers 库还提供了许多其他的功能，例如生成文本、计算相似度等。</p>
<p>Transformers 常用文档:</p>
<ul class="lvl-0">
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://github.com/huggingface/transformers/tree/main/src/transformers/models">Support of transformers model</a></p>
</li>
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/main/en/serialization">Export to ONNX</a></p>
</li>
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/main/en/torchscript">Export to TorchScript</a></p>
</li>
</ul>
<h4 id="export-to-ONNX">export to ONNX</h4>
<p>导出 ONNX 推荐两种方式.</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>使用官方推荐<a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/main/en/serialization">方法</a>导出.</p>
<ul class="lvl-2">
<li class="lvl-4">使用 optimum cli 导出 onnx, 需要模型在 optimum 中配置绑定导出映射.</li>
</ul>
</li>
<li class="lvl-2">
<p>手动配置<a target="_blank" rel="noopener" href="https://wuwt.me/2021/12/29/triton/">绑定</a>导出.</p>
</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> XLMRobertaForSequenceClassification, XLMRobertaTokenizer</span><br><span class="line"><span class="keyword">import</span> transformers</span><br><span class="line"></span><br><span class="line">tokenizer = XLMRobertaTokenizer.from_pretrained(<span class="string">"joeddav/xlm-roberta-large-xnli"</span>)</span><br><span class="line">premise = <span class="string">"Juiter's Biggest Started as Tiny Grains of Hail"</span></span><br><span class="line">hypothesis = <span class="string">"This text is about space &amp; cosmos"</span></span><br><span class="line"></span><br><span class="line">input_ids = tokenizer.encode(</span><br><span class="line">    premise,</span><br><span class="line">    hypothesis,</span><br><span class="line">    <span class="comment"># return_tensors="pt",</span></span><br><span class="line">    return_tensors=transformers.TensorType.PYTORCH,</span><br><span class="line">    max_length=<span class="number">256</span>,</span><br><span class="line">    truncation=<span class="literal">True</span>,</span><br><span class="line">    <span class="comment"># padding="max_length",</span></span><br><span class="line">    padding=transformers.utils.PaddingStrategy.MAX_LENGTH,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">mask = input_ids != <span class="number">1</span></span><br><span class="line">mask = mask.long()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 手动导出 onnx</span></span><br><span class="line">torch.onnx.export(</span><br><span class="line">    model,  <span class="comment"># 模型</span></span><br><span class="line">    [input_ids, mask],  <span class="comment"># 输入张量</span></span><br><span class="line">    <span class="string">"model.onnx"</span>,  <span class="comment"># 导出文件名</span></span><br><span class="line">    export_params=<span class="literal">True</span>,  <span class="comment"># 导出模型参数</span></span><br><span class="line">    opset_version=<span class="number">12</span>,  <span class="comment"># ONNX运算符集版本</span></span><br><span class="line">    do_constant_folding=<span class="literal">True</span>,  <span class="comment"># 是否执行常量折叠优化</span></span><br><span class="line">    input_names=[<span class="string">"input"</span>],  <span class="comment"># 输入张量的名称</span></span><br><span class="line">    output_names=[<span class="string">"output"</span>],  <span class="comment"># 输出张量的名称</span></span><br><span class="line">    dynamic_axes={<span class="string">"input"</span>: {<span class="number">0</span>: <span class="string">"batch_size"</span>}, <span class="string">"output"</span>: {<span class="number">0</span>: <span class="string">"batch_size"</span>}},  <span class="comment"># 批处理维度</span></span><br><span class="line">)</span><br><span class="line"><span class="comment"># 手动导出 pt</span></span><br><span class="line">traced_model = torch.jit.trace(model, [input_ids, mask], strict=<span class="literal">False</span>)</span><br><span class="line">torch.jit.save(traced_model, <span class="string">"model.pt"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Pytorch_to_TorchScript</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Pytorch_to_TorchScript, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.model = XLMRobertaForSequenceClassification.from_pretrained(</span><br><span class="line">            <span class="string">"joeddav/xlm-roberta-large-xnli"</span>, torchscript=<span class="literal">True</span></span><br><span class="line">        ).cuda()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, data, attention_mask=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="comment"># 手动设置输入输出层</span></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.model(data.cuda(), attention_mask.cuda())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 增加一层手动引导forward</span></span><br><span class="line">pt_model = Pytorch_to_TorchScript().<span class="built_in">eval</span>()</span><br><span class="line">traced_script_model = torch.jit.trace(pt_model, [input_ids, mask])</span><br><span class="line">torch.jit.save(traced_script_model, <span class="string">"model.pt"</span>)</span><br></pre></td></tr></tbody></table></figure>
<h2 id="mlflow">mlflow</h2>
<hr>
<p><a target="_blank" rel="noopener" href="https://mlflow.org/docs/latest/index.html">MLFlow</a> 是一个管理端到端机器学习生命周期的开源平台。</p>
<p>参考链接:</p>
<ul class="lvl-0">
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://www.mlflow.org/docs/latest/rest-api.html">https://www.mlflow.org/docs/latest/rest-api.html</a></p>
</li>
</ul>
<!-- https://github.com/Noodle-ai/mlflow_part3_PostgresMinioRegistry -->
<h3 id="mlflow-tracking">mlflow tracking</h3>
<p><a target="_blank" rel="noopener" href="https://mlflow.org/docs/latest/tracking.html#">mlflow tracking</a> 包含有后端元数据存储 (保存训练及实验数据) 和模型存储 (保存模型)</p>
<p>参考链接:</p>
<ul class="lvl-0">
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://mlflow.org/docs/latest/tracking.html#how-runs-and-artifacts-are-recorded">https://mlflow.org/docs/latest/tracking.html#how-runs-and-artifacts-are-recorded</a></p>
</li>
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/527444716">后端元数据存储及模型存储</a></p>
</li>
</ul>
<h4 id="数据保存在本地">数据保存在本地</h4>
<p>本地使用 tracking.</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>后端元数据存储 meta data 和模型存储 artifacts (mlartifact 等) 都在本地文件夹下.</p>
</li>
<li class="lvl-2">
<p>默认端口 localhost:5000.</p>
</li>
</ul>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mlflow server --dev</span><br></pre></td></tr></tbody></table></figure>
<h4 id="数据保存在database及minio">数据保存在 database 及 minio</h4>
<p>常用的生产环境下的 mlflow tracking 配置</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>mlflow: mlflow tacking 服务</p>
</li>
<li class="lvl-2">
<p>minio: 持久化 tracking 的 artifacts.</p>
</li>
<li class="lvl-2">
<p>mysql: 持久化 tracking 的 meta data.</p>
</li>
</ul>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> AWS_ACCESS_KEY_ID=...</span><br><span class="line"><span class="built_in">export</span> AWS_SECRET_ACCESS_KEY=...</span><br><span class="line"><span class="built_in">export</span> MLFLOW_S3_ENDPOINT_URL=http://localhost:9001 <span class="comment"># s3 store url</span></span><br><span class="line"><span class="built_in">export</span> ARTIFACE_S3_ENDPOINT=s3://mlflow</span><br><span class="line"><span class="built_in">export</span> MYSQL_USER=...</span><br><span class="line"><span class="built_in">export</span> MYSQL_PASSWORD=...</span><br><span class="line"><span class="built_in">export</span> MYSQL_ENDPOINT=mysql://<span class="variable">$MYSQL_USER</span>:<span class="variable">$MYSQL_PASSWORD</span>@localhost/dbname</span><br><span class="line"><span class="built_in">export</span> MLFLOW_PORT=5000</span><br><span class="line"><span class="built_in">export</span> MLFLOW_HOST=localhost</span><br><span class="line">mlflow server \</span><br><span class="line">   --backend-store-uri <span class="variable">$MYSQL_ENDPOINT</span> \</span><br><span class="line">   --host <span class="variable">$MLFLOW_HOST</span> -p <span class="variable">$MLFLOW_PORT</span> \</span><br><span class="line">   --default-artifact-root <span class="variable">$ARTIFACE_S3_ENDPOINT</span></span><br></pre></td></tr></tbody></table></figure>
<p>mlflow docker-compose 配置参考</p>
<ul class="lvl-0">
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://github.com/mlflow/mlflow/blob/master/examples/mlflow_artifacts/docker-compose.yml">https://github.com/mlflow/mlflow/blob/master/examples/mlflow_artifacts/docker-compose.yml</a></p>
</li>
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://github.com/msclock/mlflow_tritonserver/blob/main/deploy/docker/docker-compose.yml">https://github.com/msclock/mlflow_tritonserver/blob/main/deploy/docker/docker-compose.yml</a></p>
</li>
</ul>
<h3 id="mlflow-registry">mlflow registry</h3>
<p>MLflow 模型注册中心组件是一个集中的模型存储、一组 API 和 UI，用于协作管理 MLflow 模型的整个生命周期。它提供了模型谱系 (MLflow 实验和运行产生的模型)、模型版本化、阶段转换 (例如从阶段到生产) 和注释。</p>
<p>参考链接:</p>
<ul class="lvl-0">
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://www.mlflow.org/docs/latest/model-registry.html">https://www.mlflow.org/docs/latest/model-registry.html</a></p>
</li>
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://www.mlflow.org/docs/latest/model-registry.html#registering-an-unsupported-machine-learning-model">https://www.mlflow.org/docs/latest/model-registry.html#registering-an-unsupported-machine-learning-model</a></p>
</li>
</ul>
<h3 id="mlflow-workflow">mlflow workflow</h3>
<!-- https://blog.csdn.net/sinat_26917383/article/details/114134509 -->
<p>要使用 MLflow Project 部署模型，可以使用以下步骤：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>创建 MLflow 项目：创建一个包含模型训练和部署代码的 MLflow 项目。在项目目录中，应该包含一个 MLproject 文件，其中定义了项目的名称、入口点和依赖项。例如：</p>
</li>
</ul>
<figure class="highlight txt"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">name: my_project</span><br><span class="line"></span><br><span class="line">entry_points:</span><br><span class="line">    train:</span><br><span class="line">    command: "python train.py"</span><br><span class="line">    parameters:</span><br><span class="line">        data_path: {type: str, default: data.csv, description: "Path to training data"}</span><br><span class="line">        max_depth: {type: int, default: 5, description: "Maximum depth of decision tree"}</span><br><span class="line"></span><br><span class="line">    serve:</span><br><span class="line">    command: "python serve.py"</span><br><span class="line">    parameters:</span><br><span class="line">        model_uri: {type: str, default: "runs:/&lt;run_id&gt;/model", description: "URI of the trained model"}</span><br><span class="line">        port: {type: int, default: 5000, description: "Port to listen on"}</span><br></pre></td></tr></tbody></table></figure>
<p>在这个示例中，定义了两个入口点：train 和 serve。train 入口点用于训练模型，serve 入口点用于部署模型。train 入口点接受两个参数：data_path 和 max_depth。serve 入口点接受两个参数：model_uri 和 port。</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>训练模型：在 MLflow 项目目录中运行 mlflow run 命令来训练模型。例如：</p>
</li>
</ul>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mlflow run .</span><br></pre></td></tr></tbody></table></figure>
<ul class="lvl-0">
<li class="lvl-2">
<p>部署模型：在 MLflow 项目目录中运行 mlflow models serve 命令来部署模型。例如：</p>
</li>
</ul>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mlflow models serve -m runs:/&lt;run_id&gt;/model --port 5000</span><br></pre></td></tr></tbody></table></figure>
<p>这将运行 serve 入口点，并将训练的模型部署到本地端口 5000 上。MLflow 会自动加载模型，并将 HTTP 请求转发到模型的预测函数中。</p>
<p>如果要使用自定义 Python 脚本部署模型，可以使用以下步骤：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>训练模型：使用 Python 脚本训练模型，并将模型保存到本地文件中。例如：</p>
</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> mlflow</span><br><span class="line"><span class="keyword">import</span> my_model</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">model = my_model.train()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存模型</span></span><br><span class="line">mlflow.pytorch.save_model(model, <span class="string">"my_model"</span>)</span><br></pre></td></tr></tbody></table></figure>
<p>在这个示例中，使用 my_model 模块中的 train 函数训练模型，并使用 MLflow 将模型保存到名为 my_model 的文件夹中。</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>部署模型：使用 Python 脚本加载模型，并将模型部署到 Web 服务器上。例如：</p>
</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> mlflow.pytorch</span><br><span class="line"><span class="keyword">import</span> my_model</span><br><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> Flask, request, jsonify</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载模型</span></span><br><span class="line">model = mlflow.pytorch.load_model(<span class="string">"my_model"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建Flask应用程序</span></span><br><span class="line">app = Flask(__name__)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义预测函数</span></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">"/predict"</span>, methods=[<span class="string">"POST"</span>]</span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">predict</span>():</span><br><span class="line">    data = request.json</span><br><span class="line">    <span class="built_in">input</span> = my_model.preprocess(data)</span><br><span class="line">    output = model(<span class="built_in">input</span>)</span><br><span class="line">    result = my_model.postprocess(output)</span><br><span class="line">    <span class="keyword">return</span> jsonify(result)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行应用程序</span></span><br><span class="line">app.run(host=<span class="string">"0.0.0.0"</span>, port=<span class="number">5000</span>)</span><br></pre></td></tr></tbody></table></figure>
<p>在这个示例中，使用 MLflow 加载名为 my_model 的模型，并将其部署到 Flask Web 服务器上。定义了一个 /predict 端点，用于接收 POST 请求，并将请求数据传递给模型的预测函数。预测函数将输入数据预处理、调用模型进行推理、并将输出数据后处理，最终返回 JSON 格式的结果。使用 Flask 的 run 方法运行 Web 服务器，并将其绑定到本地 IP 地址和端口 5000 上。</p>
<h3 id="mlflow-with-transformers">mlflow with transformers</h3>
<h4 id="bert-sequence-classification">bert sequence classification</h4>
<p>Step 1, 安装依赖</p>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install transformers datasets mlflow</span><br></pre></td></tr></tbody></table></figure>
<p>Step 2, 结合 transformers 配置基于 mlflow 脚本</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> mlflow</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments</span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_model</span>():</span><br><span class="line">    model_name = <span class="string">"bert-base-uncased"</span></span><br><span class="line">    num_labels = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    tokenizer = AutoTokenizer.from_pretrained(model_name)</span><br><span class="line">    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)</span><br><span class="line"></span><br><span class="line">    dataset = load_dataset(<span class="string">"imdb"</span>, split=<span class="string">"train[:5%]"</span>)  <span class="comment"># 只使用5%的数据进行训练</span></span><br><span class="line">    train_dataset = dataset.<span class="built_in">map</span>(<span class="keyword">lambda</span> e: tokenizer(e[<span class="string">'text'</span>], truncation=<span class="literal">True</span>, padding=<span class="string">'max_length'</span>), batched=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    training_args = TrainingArguments(</span><br><span class="line">        output_dir=<span class="string">"./results"</span>,</span><br><span class="line">        num_train_epochs=<span class="number">1</span>,</span><br><span class="line">        per_device_train_batch_size=<span class="number">4</span>,</span><br><span class="line">        logging_dir=<span class="string">'./logs'</span>,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    trainer = Trainer(</span><br><span class="line">        model=model,</span><br><span class="line">        args=training_args,</span><br><span class="line">        train_dataset=train_dataset,</span><br><span class="line">        tokenizer=tokenizer,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> mlflow.start_run():</span><br><span class="line">        trainer.train()</span><br><span class="line"></span><br><span class="line">        mlflow.log_param(<span class="string">"model_name"</span>, model_name)</span><br><span class="line">        mlflow.log_param(<span class="string">"num_labels"</span>, num_labels)</span><br><span class="line">        mlflow.log_param(<span class="string">"num_train_epochs"</span>, training_args.num_train_epochs)</span><br><span class="line">        mlflow.log_param(<span class="string">"per_device_train_batch_size"</span>, training_args.per_device_train_batch_size)</span><br><span class="line"></span><br><span class="line">        mlflow.pytorch.log_model(trainer.model, <span class="string">"model"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    train_model()</span><br></pre></td></tr></tbody></table></figure>
<p>执行训练</p>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python train_model.py</span><br></pre></td></tr></tbody></table></figure>
<h3 id="mlflow-plugins">mlflow plugins</h3>
<p>mlflow 作为机器学习的框架无关工具，MLflow Python API 为编写与不同机器学习框架和后端集成的插件提供了开发人员 API. <a target="_blank" rel="noopener" href="https://www.mlflow.org/docs/2.4.2/plugins.html#mlflow-plugins">mlflow plugins</a> 为第三方工具集成提供了强大的自定义接口.</p>
<p>参考链接:</p>
<ul class="lvl-0">
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://github.com/mlflow/mlflow/tree/master/tests/resources/mlflow-test-plugin">https://github.com/mlflow/mlflow/tree/master/tests/resources/mlflow-test-plugin</a></p>
</li>
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://github.com/mlflow/mlflow-torchserve">torchserve deployment plugin</a></p>
</li>
</ul>
<h2 id="MLServer">MLServer</h2>
<p><a target="_blank" rel="noopener" href="https://mlserver.readthedocs.io/en/latest/index.html">MLServer</a></p>
<h2 id="alibi">alibi</h2>
<h2 id="DeepStream">DeepStream</h2>
<p><a target="_blank" rel="noopener" href="https://developer.nvidia.com/deepstream-sdk">DeepStream</a> 是一个基于 GStreamer 的 SDK，用于创建用于图像处理和目标检测处理的人工智能视觉应用程序。</p>
<ul class="lvl-0">
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://developer.nvidia.com/deepstream-getting-started">Get Started</a></p>
</li>
</ul>
<h2 id="Tritonserver">Tritonserver</h2>
<p><a target="_blank" rel="noopener" href="https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tritonserver">Nvidia Triton Inference Sever</a> 提供了兼容 CPUs 和 GPUs 运行在云端及边缘设备将模型服务化的优化推理方案。Nvidia 这里介绍了使用 Triton 快速部署可伸缩的模型的<a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/fast-and-scalable-ai-model-deployment-with-nvidia-triton-inference-server/">优势</a>。</p>
<!--
https://blog.csdn.net/sgyuanshi/article/details/123536579
https://zhuanlan.zhihu.com/p/594107043

triton learning on bilibili
https://space.bilibili.com/1320140761/channel/collectiondetail?sid=493256

yolov5 to triton 1，2，3，4
https://blog.csdn.net/weixin_41796280/article/details/125784587

Dynamic batch
https://zhuanlan.zhihu.com/p/569807754
https://zhuanlan.zhihu.com/p/569807754
https://blog.csdn.net/qq128252/article/details/127105463

Perf Analyzer
https://github.com/triton-inference-server/client/blob/main/src/c%2B%2B/perf_analyzer/docs/input_data.md
-->
<h3 id="Tutorials">Tutorials</h3>
<p>Triton Server 提供了对应的 <a target="_blank" rel="noopener" href="https://github.com/triton-inference-server/tutorials">Tutorials</a>。</p>
<ul class="lvl-0">
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://github.com/triton-inference-server/tutorials/tree/main/Conceptual_Guide/Part_1-model_deployment">Model Deployment</a>: 图片文字检测部署 Demo，包括文字区域检测和文字识别。</p>
<ul class="lvl-2">
<li class="lvl-4"><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/frameworks/index.html">Deeplearning Optimized Frameworks</a>: 包括了测试用的 TF，PT 等框架发布信息。</li>
</ul>
</li>
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://github.com/triton-inference-server/tutorials/tree/main/Conceptual_Guide/Part_2-improving_resource_utilization">Improving Resource Utilization</a>: 使用动态批处理和并发模型执行提高资源利用率。</p>
<ul class="lvl-2">
<li class="lvl-4"><a target="_blank" rel="noopener" href="https://github.com/triton-inference-server/tutorials/tree/main/Conceptual_Guide/Part_2-improving_resource_utilization#what-is-dynamic-batching">dynamic_batching</a> in config.pbtxt</li>
<li class="lvl-4"><a target="_blank" rel="noopener" href="https://github.com/triton-inference-server/tutorials/tree/main/Conceptual_Guide/Part_2-improving_resource_utilization#concurrent-model-execution">instance_group</a> in config.pbtxt</li>
</ul>
</li>
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://github.com/triton-inference-server/tutorials/tree/main/Conceptual_Guide/Part_3-optimizing_triton_configuration">Customizing Deployment With Model Analyzer</a>: 推理性能讨论及使用 Model Analyzer 优化 config.pbtxt 配置文件。</p>
<ul class="lvl-2">
<li class="lvl-4"><a target="_blank" rel="noopener" href="https://github.com/triton-inference-server/tutorials/tree/main/Conceptual_Guide/Part_3-optimizing_triton_configuration#performance-discussion">推理性能优化</a>
<ul class="lvl-4">
<li class="lvl-6"> Minimizing network latency，减小模型精度，例如，图片转换 float32 转为 float 16。</li>
<li class="lvl-6">Accelerating model, 融合网络层来优化网络图，降低模型精度，融合内核等等减少计算时间。</li>
<li class="lvl-6">Using Model Analyzer, 使用 Model Analyzer 减少在模型队列等待到推理框架调度的延时。</li>
</ul>
</li>
<li class="lvl-4"><a target="_blank" rel="noopener" href="https://github.com/triton-inference-server/tutorials/tree/main/Conceptual_Guide/Part_3-optimizing_triton_configuration#using-model-analyzer">Using Model Analyzer</a>
<ul class="lvl-4">
<li class="lvl-6"><a target="_blank" rel="noopener" href="https://github.com/triton-inference-server/model_analyzer/blob/main/docs/config.md">Analyzer Criteria</a> 包含的检测目标 Objectives 和限制 Constraints。</li>
<li class="lvl-6"><a target="_blank" rel="noopener" href="https://github.com/triton-inference-server/tutorials/tree/main/Conceptual_Guide/Part_3-optimizing_triton_configuration#usage-details">Usage Details</a> 实际使用示例。</li>
<li class="lvl-6"><a target="_blank" rel="noopener" href="https://github.com/triton-inference-server/model_analyzer">Model Analyzer Repos</a> 工具仓库包含了对单模型和多模型评估示例。</li>
</ul>
</li>
</ul>
</li>
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://github.com/triton-inference-server/tutorials/tree/main/Conceptual_Guide/Part_4-inference_acceleration">Accelerating Inference for Models</a>: 通过加速推理框架来加速推理。</p>
<ul class="lvl-2">
<li class="lvl-4"><a target="_blank" rel="noopener" href="https://github.com/triton-inference-server/tutorials/tree/main/Conceptual_Guide/Part_4-inference_acceleration#gpu-based-acceleration">GPU Based Acceleration</a>: 基于 GPU 加速
<ul class="lvl-4">
<li class="lvl-6"><a target="_blank" rel="noopener" href="https://github.com/triton-inference-server/tutorials/tree/main/Conceptual_Guide/Part_4-inference_acceleration#using-tensorrt-directly">Using TensorRT Directly</a> 直接将模型转为 TensorRT Engine 来加速。</li>
<li class="lvl-6"><a target="_blank" rel="noopener" href="https://github.com/triton-inference-server/tutorials/tree/main/Conceptual_Guide/Part_4-inference_acceleration#using-tensorrts-integration-with-pytorchtensorflow">Using TensorRT’s Integration with PyTorch/TensorFlow</a> 使用与 <a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=TU5BMU6iYZ0">PyTorch</a>/TensorFlow 集成的 TensorRT 转换。
<ul class="lvl-6">
<li class="lvl-8"><a target="_blank" rel="noopener" href="https://pytorch.org/TensorRT/tutorials/serving_torch_tensorrt_with_triton.html">PyTorch</a></li>
<li class="lvl-8"><a target="_blank" rel="noopener" href="https://github.com/tensorflow/tensorrt/tree/master/tftrt/triton">TensorFlow</a></li>
</ul>
</li>
<li class="lvl-6"><a target="_blank" rel="noopener" href="https://github.com/triton-inference-server/tutorials/tree/main/Conceptual_Guide/Part_4-inference_acceleration#using-tensorrts-integration-with-onnx-runtime">Using TensorRT’s Integration with ONNX RunTime</a></li>
</ul>
</li>
<li class="lvl-4"><a target="_blank" rel="noopener" href="https://github.com/triton-inference-server/tutorials/tree/main/Conceptual_Guide/Part_4-inference_acceleration#cpu-based-acceleration">CPU Based Acceleration</a> 基于 OpenVINO 加速。</li>
<li class="lvl-4"><a target="_blank" rel="noopener" href="https://github.com/triton-inference-server/tutorials/tree/main/Conceptual_Guide/Part_4-inference_acceleration#accelerating-large-transformer-models">Accelerating Large Transformer Models</a> 加速大模型。</li>
<li class="lvl-4"><a target="_blank" rel="noopener" href="https://github.com/triton-inference-server/model_navigator">Model Navigator</a> 检验模型转换覆盖及简化部署流程的工具。</li>
</ul>
</li>
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://github.com/triton-inference-server/tutorials/tree/main/Conceptual_Guide/Part_5-Model_Ensembles">Executing Multiple Models With Model Ensembles</a> 将多个模型组合进行推理，使用前面得文字检测，文字识别，分别对两个模型配置前后处理得 python backend，然后串联形成在 Triton Server 端运行得组合模型推理服务。</p>
</li>
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://github.com/triton-inference-server/tutorials/tree/main/Conceptual_Guide/Part_6-building_complex_pipelines">Building Complex Pipelines:Stable Diffusion</a>: 使用多个框架后端构件推理服务，并且使用 BLS API 构件复杂得非线性管道。</p>
</li>
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://github.com/triton-inference-server/tutorials/blob/main/Feature_Guide/Data_Pipelines/README.md">Data Pipeline</a>: Triton 中 Python backend 对 Tensor 的处理步骤。</p>
<ul class="lvl-2">
<li class="lvl-4">Model Inspect Tool: <a target="_blank" rel="noopener" href="https://netron.app/">Netron</a>, <a target="_blank" rel="noopener" href="https://github.com/NVIDIA/TensorRT/tree/main/quickstart/deploy_to_triton">Polygraphy</a></li>
</ul>
</li>
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/triton-inference-server/release-notes/index.html">Release Notes</a></p>
</li>
</ul>
<h3 id="Deploying-Using-Triton">Deploying Using Triton</h3>
<p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/performance_tuning.html">Deploying Using Triton</a></p>
<h3 id="Triton-Configuration"><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/model_configuration.html">Triton Configuration</a></h3>
<ul class="lvl-0">
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/TensorRT/tree/main/quickstart/deploy_to_triton">polygraphy</a> 用于确定模型的输入输出，更多示例<a target="_blank" rel="noopener" href="https://github.com/NVIDIA/TensorRT/tree/main/tools/Polygraphy/examples/cli/inspect">参考</a>。</p>
</li>
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://netron.app/">Netron</a>：在线模型查看工具</p>
</li>
</ul>
<h3 id="Model-Repository">Model Repository</h3>
<p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/model_repository.html">Model Repository</a></p>
<h3 id="Jetson-and-JetPack-Support">Jetson and JetPack Support</h3>
<p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/jetson.html">Jetson and JetPack Support</a></p>
<h3 id="HTTP-and-GRPC-Protocol-Support">HTTP and GRPC Protocol Support</h3>
<p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/protocol/README.html">HTTP and GRPC Protocol Support</a>: Triton Server 支持的各种 HTTP/REST 和 GRPC 协议扩展。参考使用示例:</p>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># --------------- logging extension -----------------</span></span><br><span class="line"><span class="comment"># 设置 log verbose 为 1</span></span><br><span class="line">curl -s -w <span class="string">'\n%{http_code}\n'</span> -d <span class="string">'{"log_verbose_level":1}'</span> -X POST localhost:8000/v2/logging</span><br><span class="line"><span class="comment"># 获取 log config</span></span><br><span class="line">curl -s localhost:8000/v2/logging | jq</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># -------------- repository extension ---------------</span></span><br><span class="line"><span class="comment"># 查看model</span></span><br><span class="line">curl -X POST localhost:8000/v2/repository/index | jq</span><br><span class="line"><span class="comment"># 加载/卸载指定模型</span></span><br><span class="line">curl -X POST localhost:8000/v2/repository/models/<span class="variable">${MODEL_NAME}</span>/load | jq</span><br><span class="line">curl -X POST localhost:8000/v2/repository/models/<span class="variable">${MODEL_NAME}</span>/unload | jq</span><br></pre></td></tr></tbody></table></figure>
<h3 id="Optimization">Optimization</h3>
<p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/optimization.html">Optimization</a><br>
- <a target="_blank" rel="noopener" href="https://github.com/triton-inference-server/client/blob/main/src/c%2B%2B/perf_analyzer/README.md">Perf Analyzer</a>: 模型度量评估工具，针对部署在 server 中的模型能对吞吐量，GPU 利用率，时延等等做一个详细的评估输出.<br>
- <a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/model_analyzer.html">Model Analyzer</a>: 该工具会自动使用 perf_analyzer 对模型做一个全方位的评估，并能输出详细的 html/pdf 报告.</p>
<h3 id="Backend">Backend</h3>
<p><a target="_blank" rel="noopener" href="https://github.com/triton-inference-server/python_backend">Python backend</a></p>
<h3 id="Tips-3">Tips</h3>
<h4 id="Model-format">Model format</h4>
<p><code>.ckpt</code> 是 tensorflow 框架下保存的模型，包含以下几个子文件：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>model.ckpt.meta ：保存 Tensorflow 计算图结构，可以理解为神经网络的网络结构</p>
</li>
<li class="lvl-2">
<p>model.ckpt ：保存 Tensorflow 程序中每一个变量的取值，变量是模型中可训练的部分</p>
</li>
<li class="lvl-2">
<p>checkpoint ：保存一个目录下所有模型文件列表</p>
</li>
</ul>
<p><code>.onnx</code> 是一种针对机器学习所设计的开放式的文件格式，用于存储训练好的模型。它使得不同的深度学习框架（如 Pytorch, MXNet）可以采用相同格式存储模型数据。简而言之，ONNX 是一种便于在各个主流深度学习框架中迁移模型的中间表达格式.</p>
<p><code>.pt</code>/<code>.torchscript</code> 是 ptorch 模型框架保存格式.</p>
<h4 id="Triton-REST-API">Triton REST API</h4>
<p>常用的 Triton 的 REST API</p>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 检测 triton 健康</span></span><br><span class="line">curl -v localhost:8000/v2/health/ready</span><br><span class="line"><span class="comment"># 模型 name 配置</span></span><br><span class="line">curl -v localhost:8000/v2/models/name/config | jq</span><br></pre></td></tr></tbody></table></figure>
<h4 id="查看模型输入输出">查看模型输入输出</h4>
<ul class="lvl-0">
<li class="lvl-2">
<p>使用 polygraphy</p>
</li>
</ul>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">python -m pip install colored polygraphy --extra-index-url https://pypi.ngc.nvidia.com</span><br><span class="line"><span class="comment"># 确定模型的输入输出</span></span><br><span class="line">POLYGRAPHY_AUTOINSTALL_DEPS=1 polygraphy inspect model model_repository/name/1/model.plan</span><br></pre></td></tr></tbody></table></figure>
<ul class="lvl-0">
<li class="lvl-2">
<p>使用 netron</p>
</li>
</ul>
<h4 id="Yolov5-pt-to-ONNX">Yolov5 pt to ONNX</h4>
<p>将模型转换为 onnx，基于 yolo 模型，使用 yolo 仓库提供的 export 脚本</p>
<blockquote>
<p>[!IMPORTANT]<br>
若需要支持 batch 推理，则需要打开 --dynamic 选项。</p>
</blockquote>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker run -it --gpus all -v $(<span class="built_in">pwd</span>):/workspace nvcr.io/nvidia/pytorch:23.04-py3</span><br><span class="line">git <span class="built_in">clone</span> https://github.com/ultralytics/yolov5.git &amp;&amp; <span class="built_in">cd</span> yolov5</span><br><span class="line">python export.py --include onnx --weights model.pt --img 1280 --dynamic --device=0</span><br></pre></td></tr></tbody></table></figure>
<p>将模型保存在需要的位置，并补充模型配置文件 config.pbtxt，如下：</p>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tree model_repository/model</span><br><span class="line">model_repository/model</span><br><span class="line">└── 1</span><br><span class="line">    └── model.onnx</span><br></pre></td></tr></tbody></table></figure>
<p>优化导出模型，将 preprocessing 和 postprocessing 加入到模型图中</p>
<blockquote>
<p>[!TIP]<br>
参考 yolov5-rt 自动化转化前后处理到模型中。</p>
</blockquote>
<!--
https://github.com/zhiqwang/yolov5-rt-stack
https://github.com/bug-developer021/YOLOV5_optimization_on_triton/blob/main/docs/batchedNMS.md
-->
<h4 id="ALBERT-to-ONNX">ALBERT to ONNX</h4>
<!--

https://github.com/onnx/tensorflow-onnx
https://github.com/chainyo/transformers-pipeline-onnx/blob/main/ner_pipeline.ipynb -->
<p>使用 <a target="_blank" rel="noopener" href="https://github.com/onnx/tensorflow-onnx">tf2onnx</a> 转换，转换命令<a target="_blank" rel="noopener" href="https://github.com/onnx/tensorflow-onnx#getting-started">参考</a></p>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pip install -U tf2onnx</span><br><span class="line">docker run -it --gpus all -v $(<span class="built_in">pwd</span>):/workspace nvcr.io/nvidia/tensorflow:23.04-tf2-py3</span><br><span class="line">python -m tf2onnx.convert --checkpoint  tensorflow-model-meta-file-path --output model.onnx --inputs input0:0,input1:0 --outputs output0:0</span><br></pre></td></tr></tbody></table></figure>
<p>参考 triton to albert <a target="_blank" rel="noopener" href="https://github.com/NVIDIA/DeepLearningExamples/blob/master/TensorFlow/LanguageModeling/BERT/triton/run_squad_triton_client.py">client</a></p>
<h4 id="Transformers-to-ONNX">Transformers to ONNX</h4>
<p>将 Transformers 模型转 ONNX <a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/serialization">参考</a></p>
<h4 id="ONNX-Patch">ONNX Patch</h4>
<p>支持 CPU/GPU 部署</p>
<blockquote>
<p>[!TIP]<br>
使用三方<a target="_blank" rel="noopener" href="https://github.com/jiangjiajun/PaddleUtils/tree/main/onnx">工具</a>可以操作 ONNX 模型裁剪、修改节点命名等。</p>
</blockquote>
<p>使用 ONNX 推理加速<a target="_blank" rel="noopener" href="https://github.com/triton-inference-server/tutorials/blob/main/Conceptual_Guide/Part_4-inference_acceleration/README.md#working-example">方案</a>。</p>
<h4 id="ONNX-转换-TensorRT-Engine">ONNX 转换 TensorRT Engine</h4>
<blockquote>
<p>[!TIP]<br>
通常 TensorRT 部署的推理速度较快。</p>
</blockquote>
<p>将 ONNX 模型转为 TensorRT Engine，仅支持 GPU 部署（需要在对应部署机器配置适配，会对生成的 TensorRT Engine 进行优化）。</p>
<blockquote>
<p>[!TIP]<br>
<code>onnx2trt_utils.cpp:374: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.</code></p>
<p>该问题可以通过 onnx-simplifier 简化 onnx 模型解决：</p>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install onnx-simplifier</span><br><span class="line">onnxsim model.onnx simplfied_model.onnx</span><br></pre></td></tr></tbody></table></figure>
</blockquote>
<p>将 ONNX 模型转为 TensorRT Engine。</p>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker run -it --gpus all -v $(<span class="built_in">pwd</span>):/trt_optimize nvcr.io/nvidia/tensorrt:23.04-py3</span><br><span class="line"><span class="built_in">cd</span> /trt_optimize</span><br><span class="line">trtexec --onnx=model.onnx --saveEngine=model.plan --useCudaGraph</span><br></pre></td></tr></tbody></table></figure>
<p>使用 Model Analyzer 评估最优配置文件 config.pbtxt</p>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">docker run -it --gpus=all --shm-size=256m --<span class="built_in">rm</span> -p8000:8000 -p8001:8001 -p8002:8002 -v $(<span class="built_in">pwd</span>):/workspace nvcr.io/nvidia/tritonserver:23.04-py3</span><br><span class="line">pip3 install triton-model-analyzer</span><br><span class="line"><span class="comment"># 输出评估缓存</span></span><br><span class="line"><span class="built_in">mkdir</span> output profile_results</span><br><span class="line"><span class="comment"># 自动评估，耗时长，这里预筛出延迟10ms的配置评估结果</span></span><br><span class="line">model-analyzer profile \</span><br><span class="line">    --model-repository model_repository \</span><br><span class="line">    --profile-models drawing_rt \</span><br><span class="line">    --triton-launch-mode=<span class="built_in">local</span> \</span><br><span class="line">    --output-model-repository-path output \</span><br><span class="line">    --override-output-model-repository \</span><br><span class="line">    --latency-budget 10 \</span><br><span class="line">    --run-config-search-mode quick</span><br><span class="line"></span><br><span class="line"><span class="comment"># 快速评估，指定参数</span></span><br><span class="line">model-analyzer profile \</span><br><span class="line">    --model-repository model_repository \</span><br><span class="line">    --profile-models drawing_rt \</span><br><span class="line">    --triton-launch-mode=<span class="built_in">local</span> \</span><br><span class="line">    --output-model-repository-path output \</span><br><span class="line">    --override-output-model-repository \</span><br><span class="line">    --export-path profile_results \</span><br><span class="line">    --run-config-search-max-concurrency 2 \</span><br><span class="line">    --run-config-search-max-model-batch-size 2 \</span><br><span class="line">    --run-config-search-max-instance-count 2</span><br></pre></td></tr></tbody></table></figure>
<p>使用 <a target="_blank" rel="noopener" href="https://towardsdatascience.com/how-to-convert-your-custom-model-into-tensorrt-5a2ea1dec2e4">TPAT</a> 转换 ONNX 自动化生成 TensorRT Engine。</p>
<h4 id="ONNX-TensorRT-Batch">ONNX/TensorRT Batch</h4>
<p>动态 Batch 处理需要，对模型的输入输出，设置为动态类型。</p>
<p>基于 yolo 的模型需要修改 <a target="_blank" rel="noopener" href="http://export.py">export.py</a> 脚本，配置输出脚本，以导出 batch 的 ONNX 模型。</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对block对应得输入，重命名</span></span><br><span class="line">dx = {</span><br><span class="line">    <span class="string">"images"</span>: {<span class="number">0</span>: <span class="string">"batch"</span>},</span><br><span class="line">    <span class="string">"output"</span>: {<span class="number">0</span>: <span class="string">"batch"</span>, <span class="number">1</span>: <span class="string">"100800"</span>, <span class="number">2</span>: <span class="string">"16"</span>},</span><br><span class="line">    <span class="string">"onnx::Sigmoid_473"</span>: {<span class="number">0</span>: <span class="string">"batch"</span>},</span><br><span class="line">    <span class="string">"onnx::Sigmoid_611"</span>: {<span class="number">0</span>: <span class="string">"batch"</span>},</span><br><span class="line">    <span class="string">"onnx::Sigmoid_748"</span>: {<span class="number">0</span>: <span class="string">"batch"</span>},</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python export.py --include onnx --weights weights/box_810.pt --img 1280 --batch-size 1 --dynamic</span><br></pre></td></tr></tbody></table></figure>
<p>或者直接使用最新 yolov5 版本导出 dynamic 版本，<a href="AI.md#yolo-%E8%BD%AC%E6%8D%A2-onnx">参考</a></p>
<p>生成 TensorRT Engine 时需要设置支持的动态维度。</p>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">trtexec --onnx=model.onnx --saveEngine=model.plan --useCudaGraph --minShapes=images:1x3x1280x1280 --optShapes=images:1x3x1280x1280 --maxShapes=images:4x3x1280x1280</span><br></pre></td></tr></tbody></table></figure>
<h4 id="dali-backend">dali_backend</h4>
<blockquote>
<p>[!NOTE]<br>
<a target="_blank" rel="noopener" href="https://github.com/triton-inference-server/dali_backend">dali_backend</a>:The Triton backend that allows running GPU-accelerated data pre-processing pipelines implemented in DALI’s python API.</p>
</blockquote>
<p>dali_backend 支持在 tritonserver 中构建 ensemble 方式构建的应用，参考<a target="_blank" rel="noopener" href="https://github.com/triton-inference-server/client#ensemble-image-classification-example-application">官方示例</a>.</p>
<h4 id="指定model推理">指定 model 推理</h4>
<p>准备推理脚本，启动服务</p>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker run -it --gpus=all --shm-size=256m --<span class="built_in">rm</span> -p8000:8000 -p8001:8001 -p8002:8002 -v $(<span class="built_in">pwd</span>):/workspace nvcr.io/nvidia/tritonserver:23.04-py3</span><br><span class="line"><span class="built_in">cd</span> /workspace</span><br><span class="line">tritonserver --model-repository=model_repository --log-verbose=1 --model-control-mode=explicit --load-model model-name</span><br></pre></td></tr></tbody></table></figure>
<p>安装依赖</p>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install tritonclient[http] opencv-python-headless</span><br><span class="line">python model_infer_client.py</span><br></pre></td></tr></tbody></table></figure>
<h4 id="Ensemble-pipeline">Ensemble(pipeline)</h4>
<blockquote>
<p>[!IMPORTANT]<br>
<code>UNAVAILABLE: Internal: Unable to initialize shared memory key 'triton_python_backend_shm_region_2' to requested size (67108864 bytes). If you are running Triton inside docker, use '--shm-size' flag to control the shared memory region size. Each Python backend model instance requires at least 64MBs of shared memory. Error: No space left on device</code></p>
<p>使用 Ensemble 模式需要加载共享内存大小，–shm-size=256m，大小根据实际需求设置。</p>
</blockquote>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">docker run -it --gpus=all --shm-size=256m --<span class="built_in">rm</span> -p8000:8000 -p8001:8001 -p8002:8002 -v $(<span class="built_in">pwd</span>):/workspace nvcr.io/nvidia/tritonserver:23.04-py3</span><br><span class="line"><span class="built_in">cd</span> /workspace/</span><br><span class="line">apt-get update &amp;&amp; apt-get install libgl1 -y --no-install-recommends &amp;&amp; pip install opencv-python opencv-python-headless</span><br><span class="line">tritonserver --model-repository=model_repository/ --log-verbose 1 --exit-on-error 0 --model-control-mode poll</span><br></pre></td></tr></tbody></table></figure>
<h4 id="Model-Warm-up">Model Warm-up</h4>
<p>模型加载，会在模型初始化时耗费时间，使用 <code>ModelWarmup</code> 可以有效规避第一次请求的花销。<a target="_blank" rel="noopener" href="https://github.com/triton-inference-server/common/blob/main/protobuf/model_config.proto"><code>ModelWarmup</code></a> 参考模型配置。官方示例<a target="_blank" rel="noopener" href="https://github.com/triton-inference-server/server/blob/main/qa/L0_warmup/test.sh">参考测试</a>。</p>
<figure class="highlight txt"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">model_warmup [</span><br><span class="line">    {</span><br><span class="line">        name: "warmup_requests"</span><br><span class="line">        batch_size: 1</span><br><span class="line">        inputs: {</span><br><span class="line">            key: "drawing_preprocessing_input"</span><br><span class="line">            value: {</span><br><span class="line">                input_data_file: "warm-up.jpg"</span><br><span class="line">                dims: [ 1507865 ]</span><br><span class="line">                data_type: TYPE_UINT8</span><br><span class="line">            }</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">]</span><br></pre></td></tr></tbody></table></figure>
<p>上面模型配置使用模型子文件夹下 warm-up.jpg，在模型加载时作为输入。</p>
<h4 id="Jetson-Compatibility">Jetson Compatibility</h4>
<!-- https://medium.com/forsight-ai/how-to-install-nvidia-dali-triton-backend-on-jetson-devices-9dea8973b699 -->
<h4 id="pytorch-with-triton">pytorch with triton</h4>
<ul class="lvl-0">
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://pytorch.org/TensorRT/tutorials/serving_torch_tensorrt_with_triton.html#">Demo</a></p>
</li>
</ul>
<h3 id="Solution">Solution</h3>
<p>结合 Triton 提供的 model 服务推理功能，可以实现如<a target="_blank" rel="noopener" href="https://github.com/triton-inference-server/server/tree/main/deploy">示例</a> 中 k8s 和 mlflow 集成解决方案。</p>
<blockquote>
<p>[!WARNING]<br>
minikube 上测试 gpu 非常麻烦，参考 <a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/62212168/cannot-use-gpu-on-minikube-with-docker-driver">stackoverflow</a> 和 <a target="_blank" rel="noopener" href="https://github.com/NVIDIA/k8s-device-plugin#nvidia-device-plugin-for-kubernetes">github</a></p>
</blockquote>
<h4 id="k8s-Integration">k8s Integration</h4>
<h4 id="mlflow-Integration">mlflow Integration</h4>
<h4 id="Customized-Server">Customized Server</h4>
<h2 id="TensorRT">TensorRT</h2>
<hr>
<p><a target="_blank" rel="noopener" href="https://developer.nvidia.com/tensorrt">TensorRT</a> 是一个高性能的深度学习推理库，可用于优化和部署深度学习模型。它使用 GPU 加速来提高推理性能，并提供了许多优化技术，例如网络剪枝、量化和层融合，以减少模型的计算和内存需求。</p>
<ul class="lvl-0">
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://developer.nvidia.com/tensorrt-getting-started#tutorials">TensorRT Tutorials</a></p>
</li>
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/quick-start-guide/index.html#deployment">TensorRT-Based Model Deploy</a></p>
</li>
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/TensorRT">TensorRT Repository</a></p>
</li>
</ul>
<!-- https://blog.csdn.net/kunhe0512/category_11813620.html -->
<h3 id="Tips-4">Tips</h3>
<h4 id="安装-2">安装</h4>
<ul class="lvl-0">
<li class="lvl-2">
<p>debian distribution <a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/install-guide/index.html#installing-debian">installation</a></p>
</li>
<li class="lvl-2">
<p>tensorrt <a target="_blank" rel="noopener" href="https://developer.nvidia.com/tensorrt">download link</a> with download</p>
</li>
</ul>
<h4 id="查看-tensorrt-版本">查看 tensorrt 版本</h4>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dpkg-query -W tensorrt</span><br><span class="line">dpkg -l | grep nvinfer</span><br><span class="line">ldconfig -p | grep nvinfer</span><br></pre></td></tr></tbody></table></figure>
<h2 id="HPC">HPC</h2>
<hr>
<p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/hpc-sdk/compilers/index.html">Nvidia HPC</a> 是一种高性能计算解决方案，旨在加速科学计算和数据分析。它使用 GPU 加速器来提高计算速度，从而使科学家和研究人员能够更快地进行计算密集型任务。</p>
<ul class="lvl-0">
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://catalog.ngc.nvidia.com/orgs/nvidia/collections/nvhpc">HPC SDK</a></p>
</li>
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://catalog.ngc.nvidia.com/orgs/hpc/collections/nvidia_hpc">HPC Collection</a></p>
</li>
</ul>
<h2 id="Cuda">Cuda</h2>
<hr>
<p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/">Nvidia Cuda</a> 简化开发人员能利用基于 Cuda 的 GPU 执行并行优化编程.</p>
<ul class="lvl-0">
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html">Cuda Programming</a></p>
</li>
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/contents.html">Cuda Best Practice</a>.</p>
</li>
</ul>
<h2 id="OpenVINO">OpenVINO</h2>
<hr>
<p><a target="_blank" rel="noopener" href="https://docs.openvino.ai/latest/home.html">OpenVINO</a> 是一个开源的优化和部署深度学习模型工具包。</p>
<h2 id="DALI">DALI</h2>
<hr>
<p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/dali/user-guide/docs/#">Nvidia DALI（Data Loading Library）</a>是一个用于数据预处理和增强的库，旨在加速深度学习模型的训练和推理。DALI 提供了高度优化的 CPU 和 GPU 数据管道，可以在数据加载和预处理方面显著提高性能。DALI 还提供了许多常用的数据增强操作，例如裁剪、缩放、旋转和翻转等，可以帮助用户更轻松地进行数据增强。在 Triton 中，DALI 可以与 PyTorch 和 TensorFlow 等深度学习框架一起使用，以加速模型的训练和推理。如果需要更快的数据加载和预处理速度，可以考虑使用 <a target="_blank" rel="noopener" href="https://github.com/NVIDIA/DALI">Nvidia DALI</a>。</p>
<p>参考<a target="_blank" rel="noopener" href="https://developer.nvidia.com/DALI">主页</a>。<a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/dali/user-guide/docs/examples/index.html">教程</a>将指导通过第一步和<a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/dali/user-guide/docs/supported_ops.html">支持的操作</a>将放在一起的 GPU 驱动的数据处理管道。</p>
<p>DALI 用户手册<a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/dali/user-guide/docs/examples/index.html">大纲</a>。</p>
<h3 id="DALI-Operation">DALI Operation</h3>
<p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/dali/user-guide/docs/examples/operations_index.html">DALI Operation</a> 包含了对常用的图形、音频、视频的处理加工教程和示例。调用接口参考 <a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/dali/user-guide/docs/supported_ops.html">Operation Interface Reference</a> 。</p>
<h3 id="安装-3">安装</h3>
<p>根据 <a target="_blank" rel="noopener" href="https://github.com/NVIDIA/DALI">DALI</a> 文档<a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/dali/user-guide/docs/installation.html">安装</a>。</p>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install --extra-index-url https://pypi.nvidia.com nvidia-dali-cuda120</span><br><span class="line">pip install --extra-index-url https://developer.download.nvidia.com/compute/redist --upgrade nvidia-dali-cuda120</span><br></pre></td></tr></tbody></table></figure>
<h3 id="Common-Process">Common Process</h3>
<h4 id="获取当前管道信息">获取当前管道信息</h4>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">cur = dali.pipeline.Pipeline.current()</span><br><span class="line"><span class="built_in">print</span>(</span><br><span class="line">    <span class="string">"\ndali config:\n"</span>,</span><br><span class="line">    <span class="string">" "</span>.join(</span><br><span class="line">        [</span><br><span class="line">            <span class="string">f"max_batch_size=<span class="subst">{cur.max_batch_size}</span>\n"</span>,</span><br><span class="line">            <span class="string">f"num_threads=<span class="subst">{cur.num_threads}</span>\n"</span>,</span><br><span class="line">            <span class="string">f"device_id=<span class="subst">{cur.device_id}</span>\n"</span>,</span><br><span class="line">        ]</span><br><span class="line">    ),</span><br><span class="line">)</span><br></pre></td></tr></tbody></table></figure>
<h3 id="Image-Preprocess">Image Preprocess</h3>
<p>适配 yolo 的 letterbox 图片预处理，将图片按原有宽高比缩小在 img_size 大小，并在 img_size 比例较大的一侧添加填充。</p>
<p>方案一: resize 后使用剪切并填充.</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义dali数据预处理管道</span></span><br><span class="line"><span class="keyword">import</span> nvidia.dali <span class="keyword">as</span> dali</span><br><span class="line"><span class="keyword">import</span> nvidia.dali.types <span class="keyword">as</span> types</span><br><span class="line"><span class="keyword">from</span> nvidia.dali.plugin.triton <span class="keyword">import</span> autoserialize</span><br><span class="line"></span><br><span class="line">IMG_SIZE = <span class="number">1280</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@autoserialize</span></span><br><span class="line"><span class="meta">@dali.pipeline_def(<span class="params">batch_size=<span class="number">256</span>, num_threads=<span class="number">9</span>, device_id=<span class="number">0</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">pipe</span>():</span><br><span class="line">    new_shape = (IMG_SIZE, IMG_SIZE)</span><br><span class="line">    images = dali.fn.external_source(device=<span class="string">"cpu"</span>, name=<span class="string">"DALI_INPUT"</span>)</span><br><span class="line">    <span class="comment"># 解码图片</span></span><br><span class="line">    images = dali.fn.decoders.image(images, device=<span class="string">"cpu"</span>, output_type=types.RGB)</span><br><span class="line">    <span class="comment"># 保持高宽比，并限制最大不超过指定的图片大小</span></span><br><span class="line">    images = dali.fn.resize(images.gpu(), size=new_shape, mode=<span class="string">"not_larger"</span>)</span><br><span class="line">    <span class="comment"># 裁剪图片，并填充超出的部分</span></span><br><span class="line">    images = dali.fn.crop_mirror_normalize(</span><br><span class="line">        images,</span><br><span class="line">        dtype=types.FLOAT,  <span class="comment"># 输出数值类型</span></span><br><span class="line">        output_layout=<span class="string">"CHW"</span>,  <span class="comment"># 输出图片的布局</span></span><br><span class="line">        fill_values=<span class="number">114.0</span> / <span class="number">255</span>,  <span class="comment"># 填充的颜色必须有效，否则无法裁剪出填充的部分</span></span><br><span class="line">        crop=new_shape,  <span class="comment"># 裁剪大小</span></span><br><span class="line">        out_of_bounds_policy=<span class="string">"pad"</span>,  <span class="comment"># 超出部分进行填充</span></span><br><span class="line">        std=[<span class="number">0.229</span> * <span class="number">255</span>, <span class="number">0.224</span> * <span class="number">255</span>, <span class="number">0.225</span> * <span class="number">255</span>],  <span class="comment"># 数值进行标准差补偿</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> images</span><br></pre></td></tr></tbody></table></figure>
<p>方案二: resize 后使用切片并填充.</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> nvidia.dali <span class="keyword">as</span> dali</span><br><span class="line"><span class="keyword">import</span> nvidia.dali.types <span class="keyword">as</span> types</span><br><span class="line"></span><br><span class="line">IMG_SIZE = <span class="number">1280.0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@pipeline_def(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="meta">    batch_size=max_batch_size,</span></span></span><br><span class="line"><span class="params"><span class="meta">    num_threads=<span class="number">4</span>,</span></span></span><br><span class="line"><span class="params"><span class="meta">    device_id=<span class="number">0</span>,</span></span></span><br><span class="line"><span class="params"><span class="meta"></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">image_process_v1</span>():</span><br><span class="line">    images = dali.fn.external_source(device=<span class="string">"cpu"</span>, name=<span class="string">"DALI_INPUT"</span>)</span><br><span class="line">    images = dali.fn.decoders.image(images, device=<span class="string">"cpu"</span>, output_type=types.RGB)</span><br><span class="line">    shapes = dali.fn.shapes(images)</span><br><span class="line">    reisze_images = dali.fn.resize(</span><br><span class="line">        images.gpu(),</span><br><span class="line">        size=(IMG_SIZE, IMG_SIZE),</span><br><span class="line">        mode=<span class="string">"not_larger"</span>,</span><br><span class="line">    )</span><br><span class="line">    resize_shape = dali.fn.shapes(reisze_images)</span><br><span class="line">    anch = dali.fn.stack(</span><br><span class="line">        (IMG_SIZE - resize_shape[<span class="number">1</span>]) / <span class="number">2</span>,</span><br><span class="line">        (IMG_SIZE - resize_shape[<span class="number">0</span>]) / <span class="number">2</span>,</span><br><span class="line">    )</span><br><span class="line">    image_processed = dali.fn.<span class="built_in">slice</span>(</span><br><span class="line">        reisze_images,</span><br><span class="line">        -anch,</span><br><span class="line">        (IMG_SIZE, IMG_SIZE),</span><br><span class="line">        fill_values=<span class="number">127</span>,</span><br><span class="line">        out_of_bounds_policy=<span class="string">"pad"</span>,</span><br><span class="line">        normalized_shape=<span class="literal">False</span>,</span><br><span class="line">        normalized_anchor=<span class="literal">False</span>,</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> (images, shapes, image_processed, anch)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">pipe = image_process_v1()</span><br><span class="line">pipe.build()</span><br><span class="line">pipe_out = pipe.run()</span><br><span class="line">show_images(pipe_out[<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(pipe_out[<span class="number">1</span>])</span><br><span class="line">show_images(pipe_out[<span class="number">2</span>])</span><br><span class="line"><span class="built_in">print</span>(pipe_out[<span class="number">3</span>])</span><br></pre></td></tr></tbody></table></figure>
<h2 id="Transformers-2">Transformers</h2>
<!--
https://www.bilibili.com/video/BV1sW4y1J7cL?p=3
https://www.youtube.com/playlist?list=PL4JICuWNgPN7hDJEIMcrhVC6Kw3vktbaA
-->
<h2 id="NLP">NLP</h2>
<!--
自然语言处理 transformer bert
 https://www.youtube.com/watch?v=wLKsaZWeuCM
transformers on nlp
 https://www.youtube.com/playlist?list=PLM2PCubgVusY4D7-R9iA6RAACNYKNXXUu
tensorflow 2.0 on nlp
 https://www.youtube.com/playlist?list=PLFI1Cd4723_SHhKoXVNHGQVQSPvvmKjTM
math of ml
 https://github.com/aespresso/a_journey_into_math_of_ml
-->
<h2 id="Yolov5">Yolov5</h2>
<ul class="lvl-0">
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/172121380">yolov5 基础</a></p>
</li>
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://start.oneflow.org/oneflow-yolo-doc/tutorials/00_chapter/overview.html">yolov5 解读</a></p>
</li>
</ul>
<h3 id="Resource">Resource</h3>
<ul class="lvl-0">
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://github.com/ultralytics/yolov5">yolov5</a>: 官方仓库.</p>
</li>
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://github.com/msclock/yolov5-utils">yolov5-utils</a>: 将官方仓库打包好的 pip pkg.</p>
</li>
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://github.com/robmarkcole/yolov5-flask">yolov5-flask</a>: yolov5 完善的 flask app.</p>
</li>
</ul>
<h3 id="输出IOU">输出 IOU</h3>
<p><code>IOU = 两个矩形交集的面积/两个矩形的并集面积</code> , 两矩形表示检测矩形 (predicted) 和标注 (ground-truth) 矩形.</p>
<p>将待测图像输入网络得到的输出结果是一个高维矩阵，<code>[-1, H, W, B, (4 + 1 + C)]</code>. 一幅图片被分割为了 H*W 块.（参看论文中的 Pr (object)∗IOU）</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>H: 表示纵向分割的块的数目</p>
</li>
<li class="lvl-2">
<p>W: 表示横向分割的块的数目</p>
</li>
<li class="lvl-2">
<p>B: 表示 anchors 的数目</p>
</li>
<li class="lvl-2">
<p>C: 表示 classes 的数目</p>
</li>
<li class="lvl-2">
<p>4:x,y,w,h，</p>
</li>
<li class="lvl-2">
<p>1:confidence</p>
</li>
</ul>
<h2 id="bert-albert">bert/albert</h2>
<ul class="lvl-0">
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/column/hellonlp">hellonlp</a></p>
<ul class="lvl-2">
<li class="lvl-4"><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/164873441">多标签文本分类 albert</a>, 参考 <a target="_blank" rel="noopener" href="https://github.com/hellonlp/classifier-multi-label/tree/master/classifier_multi_label">github</a></li>
</ul>
</li>
</ul>
<h3 id="模型资源">模型资源</h3>
<ul class="lvl-0">
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://huggingface.co/bert-base-uncased">bert-base-uncased</a>: HuggingFace bert 模型.</p>
</li>
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://huggingface.co/voidful/albert_chinese_tiny">albert_chinese_tiny</a>:HuggingFace chinese albert 模型.</p>
</li>
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://github.com/brightmart/albert_zh">albert_zh</a> 中文训练模型框架.</p>
</li>
</ul>
<!--
https://github.com/ysh329/deep-learning-model-convertor

https://blog.csdn.net/qq_33934427/article/details/123800910
https://github.com/jiangxinyang227/bert-for-task
https://www.tensorflow.org/text/tutorials/classify_text_with_bert
https://github.com/brightmart/albert_zh/tree/master
https://www.cnblogs.com/jiangxinyang/articles/10241243.html

https://www.bilibili.com/video/BV1a44y1H7Jc?p=7
https://www.bilibili.com/video/BV1JV4y1r7HP
 -->
<h2 id="LLM">LLM</h2>
<!--
https://zhuanlan.zhihu.com/p/597586623
https://www.nvidia.cn/deep-learning-ai/solutions/large-language-models/
 -->
<h3 id="ChatGLM">ChatGLM</h3>
<p><a target="_blank" rel="noopener" href="https://github.com/THUDM/ChatGLM-6B">ChatGLM</a> 是一个基于大型语言模型的对话生成模型，它可以生成自然流畅的对话，可以用于聊天机器人、智能客服等场景。ChatGLM 使用了 GPT-2 模型，该模型是由 OpenAI 开发的一种基于 Transformer 的语言模型，可以生成高质量的自然语言文本。ChatGLM-6B 是 ChatGLM 的一个版本，它使用了包含 60 亿个参数的 GPT-2 模型，可以生成更加自然流畅的对话。</p>
<p>模型相关链接:</p>
<ul class="lvl-0">
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://huggingface.co/THUDM/chatglm2-6b">huggingface chatglm2</a></p>
</li>
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://github.com/THUDM/ChatGLM-6B/blob/main/api.py">chatglm fast api demo</a></p>
</li>
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://github.com/THUDM/ChatGLM-6B/blob/main/ptuning/README.md">chatglm tuning</a></p>
</li>
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://github.com/hiyouga/ChatGLM-Efficient-Tuning">ChatGLM-Efficient-Tuning</a></p>
</li>
</ul>
<p>参考应用方案:</p>
<ul class="lvl-0">
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://github.com/imClumsyPanda/langchain-ChatGLM">基于本地知识库的 ChatGLM 问答</a></p>
</li>
</ul>
<h4 id="测试官方-Demo">测试官方 Demo</h4>
<p>依赖:</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>GPU &gt;= 6 G or CPU &gt;= 32G (实际测试 5G 即可)</p>
</li>
<li class="lvl-2">
<p>pip: <code>pip install protobuf transformers==4.30.2 cpm_kernels torch&gt;=2.0 gradio mdtex2html sentencepiece accelerate</code></p>
</li>
</ul>
<blockquote>
<p>[!TIP]<br>
Error: RuntimeError: <a target="_blank" rel="noopener" href="https://github.com/THUDM/ChatGLM-6B/issues/839">Library cuda is not initialized</a></p>
<p>Fix: <code>ln -s /usr/lib/x86_64-linux-gnu/libcuda.so /usr/local/cuda/lib64/libcuda.so</code></p>
</blockquote>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModel</span><br><span class="line"></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(</span><br><span class="line">    <span class="string">"THUDM/chatglm-6b-int4"</span>, trust_remote_code=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">model = (</span><br><span class="line">    AutoModel.from_pretrained(</span><br><span class="line">        <span class="string">"THUDM/chatglm-6b-int4"</span>,</span><br><span class="line">        trust_remote_code=<span class="literal">True</span>,</span><br><span class="line">    )</span><br><span class="line">    .half()</span><br><span class="line">    .cuda()</span><br><span class="line">)</span><br><span class="line">model = model.<span class="built_in">eval</span>()</span><br><span class="line">response, history = model.chat(tokenizer, <span class="string">"你好"</span>, history=[])</span><br><span class="line"><span class="built_in">print</span>(response)</span><br><span class="line">response, history = model.chat(tokenizer, <span class="string">"晚上睡不着应该怎么办"</span>, history=history)</span><br><span class="line"><span class="built_in">print</span>(response)</span><br></pre></td></tr></tbody></table></figure>
<h3 id="VisualGLM">VisualGLM</h3>
<p>VisualGLM-6B 是一个开源的，支持图像、中文和英文的多模态对话语言模型，语言模型基于 <a target="_blank" rel="noopener" href="https://github.com/THUDM/ChatGLM-6B">ChatGLM-6B</a>，具有 62 亿参数；图像部分通过训练 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2301.12597">BLIP2-Qformer</a> 构建起视觉模型与语言模型的桥梁，整体模型共 78 亿参数。</p>
<p>模型相关链接:</p>
<ul class="lvl-0">
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://huggingface.co/THUDM/visualglm-6b">huggingface visualglm</a></p>
</li>
</ul>
<h2 id="ImageBind">ImageBind</h2>
<p><a target="_blank" rel="noopener" href="https://github.com/facebookresearch/ImageBind">ImageBind</a> 多模态互转.</p>
<h2 id="segment-anything">segment-anything</h2>
<p><a target="_blank" rel="noopener" href="https://github.com/facebookresearch/segment-anything">segment-anything</a> MetaAI 分割模型.</p>
<h2 id="milvus">milvus</h2>
<p><a target="_blank" rel="noopener" href="https://github.com/milvus-io/milvus">milvus</a> 向量数据库 (非结构化数据库).</p>
<p>links:</p>
<ul class="lvl-0">
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://milvus.io/docs">milvus 官方文档</a></p>
</li>
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/476025527">云原生向量数据库 Milvus 扫盲</a></p>
</li>
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/634013251">是否需要一个向量数据库</a></p>
</li>
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/139847892">以图搜视频 demo</a></p>
</li>
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://github.com/milvus-io/pymilvus">milvus python sdk</a></p>
</li>
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://github.com/zilliztech/cloud-vectordb-examples">milvus/zilliz example</a></p>
</li>
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://github.com/qism/milvus_performance">milvus_performance</a></p>
</li>
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://zilliz.com/vector-database-benchmark-tool">vector database benchmark</a></p>
</li>
</ul>
<h3 id="Design">Design</h3>
<h4 id="架构">架构</h4>
<p>links:</p>
<ul class="lvl-0">
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://milvus.io/docs/architecture_overview.md">https://milvus.io/docs/architecture_overview.md</a></p>
</li>
</ul>
<h4 id="索引">索引</h4>
<p>最近邻搜索算法</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>搜索：将查询向量和搜索空间每个向量比较相似性，相似性可选择欧氏距离，维度夹角等</p>
</li>
<li class="lvl-2">
<p>优缺点：搜索质量完美，和每个向量都做比较，时间复杂度极高，仅支持小规模数据集 (百万)</p>
</li>
<li class="lvl-2">
<p>milvus: FLAT</p>
</li>
</ul>
<p>近似近邻搜索 - 聚类算法 - kmeans</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>搜索：构建索引时，选定要分类的数量，随机选择分类数量个点分别划分向量空间的每个向量，然后计算出每个分类空间中的聚类中心点 (平均向量点), 重新再分类，然后重复迭代，直到收敛。搜索时，选择查询向量距离最近的聚类中心点，比较该分类空间中的向量即可</p>
</li>
<li class="lvl-2">
<p>优化：增加聚类的数量，增加搜索的空间的数量</p>
</li>
<li class="lvl-2">
<p>优缺点：分治搜索，搜索速度和质量成反比</p>
</li>
<li class="lvl-2">
<p>milvus: IVF_FLAT,IVF_SQ8</p>
</li>
</ul>
<blockquote>
<p>[!NOTE]<br>
IVF（Inverted File）是一种用于加速近似最近邻搜索的索引结构。IVF_FLAT 是 IVF 索引的一种类型。</p>
<p>IVF 索引将向量数据集划分为多个小的聚类中心，每个聚类中心都有一个倒排文件。当进行查询时，Milvus 会根据查询向量的特征找到最相似的聚类中心，然后只在这些聚类中心的倒排文件中搜索，从而减少搜索的时间复杂度。</p>
</blockquote>
<p>投影法</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>搜索：构建索引时，对每个维度，使用易于碰撞的哈希函数映射向量空间中的向量，形成分类空间。搜索时，使用相同的哈希函数计算搜索向量得到对应的分类空间，比较分类空间中的向量即可.</p>
</li>
<li class="lvl-2">
<p>优化：将单个向量分为多段 (小维度) 向量降低分类空间形成的索引大小，增加近似搜索空间的数量</p>
</li>
<li class="lvl-2">
<p>优缺点：哈希搜索，收敛搜索更快</p>
</li>
<li class="lvl-2">
<p>milvus: ANNOY</p>
</li>
</ul>
<p>有损压缩 - 量化 - 积量化 (PQ)</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>搜索：使用分类空间中的的质心点编码代替向量存储在内存，将编码映射成码本.</p>
</li>
<li class="lvl-2">
<p>优化：将向量分段为低维度向量，对子向量进行量化，将码本的增加速率从指数降为加法</p>
</li>
<li class="lvl-2">
<p>优缺点：节省内存，搜索质量视向量聚类的稀疏而定，码本会形成性能瓶颈</p>
</li>
<li class="lvl-2">
<p>milvus: IVF_PQ</p>
</li>
</ul>
<p>基于图的搜索算法 (HNSW)</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>搜索：建立图结构，每个向量点都使用合适的方法计算出建立连接的邻节点，比如德劳内 (Delaunay) 三角剖分法.</p>
</li>
<li class="lvl-2">
<p>优化：将向量空间点，依次拿出放到搜索空间计算近邻点，以此形成类似三角剖分法路径和噪声干扰路径 (有利于相聚更远的两个向量建立连接), 即向粗后慢</p>
</li>
<li class="lvl-2">
<p>优化 2: 基于图的分层搜索算法，将前面提到的聚类质心按分类大小从大到小分层建立图结构，分层从上到下节点间连接越密集，即先保证快速导航，进入下一层精细搜索</p>
</li>
<li class="lvl-2">
<p>优缺点：搜索速度稳定，但占用内存极大，无法像积量化压缩向量，还要维护分层结构</p>
</li>
<li class="lvl-2">
<p>milvus: HNSW</p>
</li>
</ul>
<p>links:</p>
<ul class="lvl-0">
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://milvus.io/docs/index.md">https://milvus.io/docs/index.md</a></p>
</li>
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://medium.com/unstructured-data-service/how-to-choose-an-index-in-milvus-4f3d15259212">https://medium.com/unstructured-data-service/how-to-choose-an-index-in-milvus-4f3d15259212</a></p>
</li>
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://zilliz.com/learn/choosing-right-vector-index-for-your-project">https://zilliz.com/learn/choosing-right-vector-index-for-your-project</a></p>
</li>
</ul>
<h4 id="度量">度量</h4>
<ul class="lvl-0">
<li class="lvl-2">
<p>CV: Euclidean distance (L2), Inner product(IP)</p>
</li>
<li class="lvl-2">
<p>NLP: Inner product(IP), Hamming</p>
</li>
<li class="lvl-2">
<p>molecular: Jaccard</p>
</li>
</ul>
<p>links:</p>
<ul class="lvl-0">
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://milvus.io/docs/metric.md">https://milvus.io/docs/metric.md</a></p>
</li>
</ul>
<h3 id="Getting-started">Getting started</h3>
<h4 id="install-2">install</h4>
<p>使用 standalone 模式安装</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>基于 <a target="_blank" rel="noopener" href="https://milvus.io/docs/install_standalone-docker.md">docker-compose</a> 安装</p>
</li>
<li class="lvl-2">
<p>基于 <a target="_blank" rel="noopener" href="https://milvus.io/docs/install_standalone-helm.md">helm</a> 安装</p>
</li>
</ul>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">helm repo add milvus https://milvus-io.github.io/milvus-helm/ &amp;&amp; helm repo update</span><br><span class="line">helm install my-release \</span><br><span class="line">    milvus/milvus \</span><br><span class="line">    --<span class="built_in">set</span> cluster.enabled=<span class="literal">false</span> \</span><br><span class="line">    --<span class="built_in">set</span> etcd.replicaCount=1 \</span><br><span class="line">    --<span class="built_in">set</span> minio.mode=standalone \</span><br><span class="line">    --<span class="built_in">set</span> pulsar.enabled=<span class="literal">false</span></span><br><span class="line">kubectl get pod my-release-milvus-standalone-54c4f88cb9-f84pf --template=<span class="string">'{{(index (index .spec.containers 0).ports 0).containerPort}}{{"\n"}}'</span></span><br><span class="line">kubectl port-forward service/my-release-milvus 27017:19530</span><br><span class="line">helm uninstall my-release</span><br></pre></td></tr></tbody></table></figure>
<p>使用 cluser 模式</p>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">helm repo add milvus https://milvus-io.github.io/milvus-helm/ &amp;&amp; helm repo update</span><br><span class="line">helm install my-release milvus/milvus</span><br><span class="line">helm uninstall my-release</span><br></pre></td></tr></tbody></table></figure>
<h3 id="Examples">Examples</h3>
<p>处理所有非结构化数据，如反向图像搜索、音频搜索、分子搜索、视频分析、问答系统、自然语言处理等示例.</p>
<p>links:</p>
<ul class="lvl-0">
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://milvus.io/docs/example_code.md">https://milvus.io/docs/example_code.md</a></p>
</li>
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://github.com/milvus-io/bootcamp">https://github.com/milvus-io/bootcamp</a></p>
</li>
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://github.com/towhee-io/examples">https://github.com/towhee-io/examples</a></p>
</li>
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://gitlab.com/msclock/milvus_solution">milvus evaluation solution</a></p>
</li>
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://osschat.io/">milvus official osschat</a></p>
</li>
</ul>
<h2 id="Service-Encoding">Service Encoding</h2>
<p>处于安全考虑，通常需要将代码进行加密，以防止源码泄露。在 AI 服务中常见的加密方式有:</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>将源码转为 exe</p>
</li>
<li class="lvl-2">
<p>授权 license</p>
</li>
<li class="lvl-2">
<p>加密模型</p>
</li>
</ul>
<p>links:</p>
<ul class="lvl-0">
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_43508499/article/details/124390983">模型加密示例</a></p>
</li>
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/zong596568821xp/article/details/120522664">部署深度学习模型时的全流程加密方案探索</a></p>
</li>
</ul>
<h3 id="将源码转为exe">将源码转为 exe</h3>
<p>较为流程的转换方案是使用 pyinstaller 和 nuikta 将源码转为 exe. 这里使用 nuikta 方案.</p>
<p>links:</p>
<ul class="lvl-0">
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://gitlab.com/msclock/pytools/-/tree/master/nuitka">https://gitlab.com/msclock/pytools/-/tree/master/nuitka</a></p>
</li>
</ul>
<h3 id="授权license">授权 license</h3>
<h3 id="加密模型">加密模型</h3>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>Msclock
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://msclock.github.io/posts/c21efe28/" title="AI 实践">https://msclock.github.io/posts/c21efe28/</a>
  </li>
  <li class="post-copyright-license">
      <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="followme">
  <span>欢迎关注我的其它发布渠道</span>

  <div class="social-list">

      <div class="social-item">
          <a target="_blank" class="social-link" href="/atom.xml">
            <span class="icon">
              <i class="fa fa-rss"></i>
            </span>

            <span class="label">RSS</span>
          </a>
      </div>
  </div>
</div>

          <div class="post-tags">
              <a href="/tags/AI/" rel="tag"><i class="fa fa-tag"></i> AI</a>
              <a href="/tags/Triton-Serving/" rel="tag"><i class="fa fa-tag"></i> Triton Serving</a>
              <a href="/tags/AI-Serving/" rel="tag"><i class="fa fa-tag"></i> AI Serving</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/posts/d215ead/" rel="prev" title="刷新 dns">
                  <i class="fa fa-angle-left"></i> 刷新 dns
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/posts/b4d2b7f5/" rel="next" title="openvpn">
                  openvpn <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments" id="lv-container" data-id="city" data-uid="MTAyMC81Njg4MS8zMzM0NQ=="></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 2019 – 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Msclock</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>站点总字数：</span>
    <span title="站点总字数">425k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span>站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">6:27</span>
  </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/msclock" class="github-corner" title="在 GitHub 上关注我" aria-label="在 GitHub 上关注我" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>
<script src="https://cdn.jsdelivr.net/npm/darkmode-js@1.5.7/lib/darkmode-js.min.js"></script>

<script>
var options = {
  bottom: '64px',
  right: 'unset',
  left: '32px',
  time: '0.5s',
  mixColor: 'transparent',
  backgroundColor: 'transparent',
  buttonColorDark: '#100f2c',
  buttonColorLight: '#fff',
  saveInCookies: true,
  label: '🌓',
  autoMatchOsTheme: true
}
const darkmode = new Darkmode(options);
window.darkmode = darkmode;
darkmode.showWidget();
</script>
<script src="https://cdn.jsdelivr.net/npm/hexo-theme-next@8.23.0/source/js/third-party/comments/livere.min.js" defer></script>


        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 1,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z\d\-\.\+]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(t.test(e.href)||r.test(e.href))&&(e.href=a.dataset.original)})});</script><script>(r=>{r.imageLazyLoadSetting.processImages=t;var a=r.imageLazyLoadSetting.isSPA,o=r.imageLazyLoadSetting.preloadRatio||1,d=i();function i(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(t){(a||t)&&(d=i());for(var e,n=0;n<d.length;n++)0<=(e=(e=d[n]).getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(r.innerHeight*o||document.documentElement.clientHeight*o)&&(()=>{var t,e,a,o,i=d[n];e=function(){d=d.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).dataset.loaded||(t.hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(a=new Image,o=t.getAttribute("data-original"),a.onload=function(){t.src=o,t.removeAttribute("data-original"),t.setAttribute("data-loaded",!0),e&&e()},a.onerror=function(){t.removeAttribute("data-original"),t.setAttribute("data-loaded",!1),t.src=o},t.src!==o&&(a.src=o)))})()}function e(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",e),r.addEventListener("resize",e),r.addEventListener("orientationchange",e)})(this);</script></body>
</html>
